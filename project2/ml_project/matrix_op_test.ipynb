{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9a583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# loss_function   \n",
    "def loss_function(y_pred, y_target):\n",
    "     return (y_pred - y_target).pow(2).sum()\n",
    "    \n",
    "# (d loss)/(d y_pred) = 2*(y_pred - y_target)\n",
    "def loss_backwards(y_pred, y_target):\n",
    "     return 2*(y_pred - y_target)\n",
    "    \n",
    "    \n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974b049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| forward_function(x, a, b): tensor([2.3099, 1.9684], grad_fn=<AddBackward0>)\n",
      "ic| ' '\n",
      "ic| \"backprop. of a (manually):\": 'backprop. of a (manually):'\n",
      "    (loss_backwards(y_pred, y_target)@a_backwars(x, a, b)).item(): 6.5564422607421875\n",
      "ic| 'backprop. of a (with autograd):', a.grad: tensor(6.5564)\n",
      "ic| ' '\n",
      "ic| \"backprop. of a (manually):\": 'backprop. of a (manually):'\n",
      "    (torch.t(a_backwars(x, a, b))@loss_backwards(y_pred, y_target)).item(): 6.5564422607421875\n",
      "ic| 'backprop. of a (with autograd):', a.grad: tensor(6.5564)\n",
      "ic| ' '\n",
      "ic| \"backprop. of b (manually):\": 'backprop. of b (manually):'\n",
      "    (loss_backwards(y_pred, y_target)@b_backwars(x, a, b)).item(): 3.936736822128296\n",
      "ic| 'backprop. of b (with autograd):', b.grad: tensor(3.9367)\n",
      "ic| ' '\n",
      "ic| \"backprop. of b (manually):\": 'backprop. of b (manually):'\n",
      "    (torch.t(b_backwars(x, a, b))@loss_backwards(y_pred, y_target)).item(): 3.936736822128296\n",
      "ic| 'backprop. of b (with autograd):', b.grad: tensor(3.9367)\n",
      "ic| b.grad: tensor(3.9367)\n",
      "ic| ' '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# forward_function\n",
    "def forward_function(x, a, b):\n",
    "    # y1 = a+b*x1\n",
    "    # y2 = a+b*x2\n",
    "    # ....\n",
    "     return a + b * x\n",
    "    \n",
    "# (d forward_function)/(d a) = (1, 1, ...)\n",
    "def a_backwars(x, a, b):\n",
    "     return torch.ones(x.shape, dtype = dtype)\n",
    "     #return torch.tensor([1 for _ in range(len(forward_function(x, a, b)))], dtype = dtype)\n",
    "    \n",
    "# (d forward_function)/(d b) = (x1, x2, ...)\n",
    "def b_backwars(x, a, b):\n",
    "     return x\n",
    "    \n",
    "    \n",
    "#input\n",
    "x = torch.tensor([0, 1], dtype = dtype)\n",
    "#target\n",
    "y_target = torch.tensor([1, 0], dtype = dtype)\n",
    "\n",
    "#parameters\n",
    "a = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "\n",
    "    \n",
    "ic(forward_function(x, a, b))\n",
    "y_pred = forward_function(x, a, b)\n",
    "loss = loss_function(y_pred, y_target)\n",
    "\n",
    "# Use autograd to compute the backward pass. This call will compute the\n",
    "# gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "# After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "# the gradient of the loss with respect to a, b, c, d respectively.\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "ic(\" \")\n",
    "ic(\"backprop. of a (manually):\", (loss_backwards(y_pred, y_target) @ a_backwars(x, a, b)).item())\n",
    "ic(\"backprop. of a (with autograd):\", a.grad)\n",
    "ic(\" \")\n",
    "ic(\"backprop. of a (manually):\", (torch.t(a_backwars(x, a, b)) @ loss_backwards(y_pred, y_target)).item())\n",
    "ic(\"backprop. of a (with autograd):\", a.grad)\n",
    "ic(\" \")\n",
    "ic(\"backprop. of b (manually):\", (loss_backwards(y_pred, y_target) @ b_backwars(x, a, b)).item())\n",
    "ic(\"backprop. of b (with autograd):\", b.grad)\n",
    "ic(\" \")\n",
    "ic(\"backprop. of b (manually):\", (torch.t(b_backwars(x, a, b)) @ loss_backwards(y_pred, y_target)).item())\n",
    "ic(\"backprop. of b (with autograd):\", b.grad)\n",
    "ic(b.grad)\n",
    "ic(\" \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9acc10a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| loss_backwards(y_pred, y_target): tensor([-1.6441,  2.2064], grad_fn=<MulBackward0>)\n",
      "    w_backwars(x, w, b): tensor([[[-0.1499, -0.3289],\n",
      "                                  [ 0.0000,  0.0000]],\n",
      "                         \n",
      "                                 [[ 0.0000,  0.0000],\n",
      "                                  [-0.1499, -0.3289]]], grad_fn=<CopySlices>)\n",
      "ic| test: tensor([[ 0.2464,  0.5407],\n",
      "                  [-0.3306, -0.7256]], grad_fn=<AddBackward0>)\n",
      "ic| ' '\n",
      "ic| \"backprop. of w (manually):\": 'backprop. of w (manually):'\n",
      "    loss_backwards(y_pred, y_target)@w_backwars(x, w, b): tensor([[ 0.2464,  0.5407],\n",
      "                                                                  [-0.3306, -0.7256]], grad_fn=<UnsafeViewBackward0>)\n",
      "ic| \"backprop. of w (with autograd):\": 'backprop. of w (with autograd):'\n",
      "    w.grad: tensor([[ 0.2464,  0.5407],\n",
      "                    [-0.3306, -0.7256]])\n",
      "ic| ' '\n",
      "ic| \"backprop. of b (manually):\": 'backprop. of b (manually):'\n",
      "    loss_backwards(y_pred, y_target)@b_backwars(x, w, b): tensor([-1.6441,  2.2064], grad_fn=<SqueezeBackward4>)\n",
      "ic| \"backprop. of b (with autograd):\": 'backprop. of b (with autograd):'\n",
      "    b.grad: tensor([-1.6441,  2.2064])\n",
      "ic| ' '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# forward_function\n",
    "def forward_function(x, w, b):\n",
    "    # y1 = b1+w(1,1)*x1 + w(1,2)*x2\n",
    "    # y2 = b2+w(2,1)*x1 + w(2,2)*x2\n",
    "     return w @ x + b\n",
    "    \n",
    "# (d forward_function)/(d w) = (1, 1, ...)\n",
    "#def w_backwars(x, w, b):\n",
    "#     return torch.ones(len(x), dtype = dtype)\n",
    "    \n",
    "# (d forward_function)/(d b) = (x1, x2, ...)\n",
    "def b_backwars(x, w, b):\n",
    "    #(d y)/(d b) = ((d y1)/(d b), (d y2)/(d b)) \n",
    "    # = (((d y1)/(d b1), (d y1)/(d b2)), ((d y2)/(d b1), (d y2)/(d b2))) #Jacobimatrix\n",
    "    # = ((1, 0), (0, 1))\n",
    "    return torch.eye(len(b), dtype = dtype)\n",
    "\n",
    "def w_backwars(x, w, b):\n",
    "    # (d y)/(d w) = ((d y1)/(d w), (d y2)/(d w)) \n",
    "    # = ((((d y1)/(d w(1,1)), (d y1)/(d w(1,2))), ((d y1)/(d w(2,1)), (d y1)/(d w(2,2)))), (((d y2)/(d w(1,1)), (d y2)/(d w(1,2))), ((d y2)/(d w(2,1)), (d y2)/(d w(2,2))))) #Von uns veralgemeinerte Jacobimatrix (Tensor 3.stufe)\n",
    "    # = ((( x1              ,  x2              ), ( 0               ,  0               )), (( 0               ,  0               ), ( x1              ,  x2              )))\n",
    "    # = ([(x1, x2), (0,  0)], [(0,  0), (x1,  x2)])\n",
    "    # \n",
    "    c = torch.zeros(len(x), len(x), len(x), dtype=x.dtype) \n",
    "    for i in range(len(x)):\n",
    "        c[i,i] = x\n",
    "    return c\n",
    "    #return x.repeat(len(x),1) # does not work\n",
    "    \n",
    "    \n",
    "#input\n",
    "#x = torch.tensor([0, 1], dtype = dtype)\n",
    "x = torch.randn((2), dtype=dtype, requires_grad=True)\n",
    "\n",
    "#target\n",
    "#y_target = torch.tensor([1, 0], dtype = dtype)\n",
    "y = torch.randn((2), dtype=dtype, requires_grad=True)\n",
    "\n",
    "#parameters\n",
    "#w = torch.arange(4).reshape(2,2)\n",
    "#w = torch.tensor(w.clone(), dtype=dtype, requires_grad=True)\n",
    "#b = torch.arange(2, dtype=dtype, requires_grad=True)\n",
    "\n",
    "w = torch.randn((2,2), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((2), dtype=dtype, requires_grad=True)\n",
    "    \n",
    "y_pred = forward_function(x, w, b)\n",
    "loss = loss_function(y_pred, y_target)\n",
    "\n",
    "# Use autograd to compute the backward pass. This call will compute the\n",
    "# gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "# After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "# the gradient of the loss with respect to a, b, c, d respectively.\n",
    "loss.backward()\n",
    "\n",
    "ic(loss_backwards(y_pred, y_target),w_backwars(x, w, b) )\n",
    "test = w_backwars(x, w, b)[0] * loss_backwards(y_pred, y_target)[0] + w_backwars(x, w, b)[1] * loss_backwards(y_pred, y_target)[1]\n",
    "ic(test)\n",
    "ic(\" \")\n",
    "ic(\"backprop. of w (manually):\", (loss_backwards(y_pred, y_target) @ w_backwars(x, w, b)))\n",
    "ic(\"backprop. of w (with autograd):\", w.grad)\n",
    "ic(\" \")\n",
    "ic(\"backprop. of b (manually):\", (loss_backwards(y_pred, y_target) @ b_backwars(x, w, b)))\n",
    "ic(\"backprop. of b (with autograd):\", b.grad)\n",
    "#ic(b.grad)\n",
    "ic(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0512ffd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| forward_function(x, a, b): tensor([0.7106, 0.2854], grad_fn=<AddBackward0>)\n",
      "ic| ' '\n",
      "ic| \"backprop. of a (manually):\": 'backprop. of a (manually):'\n",
      "    (loss_backwards(y_pred_np, y_target_np)@a_backwars(x_np, a_np, b_np)).item(): -0.2600867767409524\n",
      "ic| 'backprop. of a (with autograd):', a.grad: tensor(-0.2601)\n",
      "ic| ' '\n",
      "ic| \"backprop. of a (manually):\": 'backprop. of a (manually):'\n",
      "    (loss_backwards(y_pred_np, y_target_np)@b_backwars(x_np, a_np, b_np)).item(): 0.1485618433179697\n",
      "ic| 'backprop. of b (with autograd):', b.grad: tensor(0.1486)\n",
      "ic| ' '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward_function\n",
    "def forward_function(x, a, b):\n",
    "    # y1 = a+b*x1\n",
    "    # y2 = a+b*x2\n",
    "    # ....\n",
    "     return a + b * x\n",
    "    \n",
    "# (d forward_function)/(d a) = (1, 1, ...)\n",
    "def a_backwars(x, a, b):\n",
    "     return np.ones(x_np.shape)\n",
    "     #return torch.tensor([1 for _ in range(len(forward_function(x, a, b)))], dtype = dtype)\n",
    "    \n",
    "# (d forward_function)/(d b) = (x1, x2, ...)\n",
    "def b_backwars(x, a, b):\n",
    "     return x\n",
    "    \n",
    "#input\n",
    "x_np = np.random.rand(2)\n",
    "x = torch.tensor(x_np, dtype = dtype)\n",
    "\n",
    "#target\n",
    "y_target_np = np.random.rand(2)\n",
    "y_target = torch.tensor(y_target_np, dtype = dtype)\n",
    "\n",
    "#parameters\n",
    "a_np = np.random.rand()\n",
    "a = torch.tensor(a_np, dtype = dtype, requires_grad=True)\n",
    "b_np = np.random.rand()\n",
    "b = torch.tensor(b_np, dtype = dtype, requires_grad=True)\n",
    "    \n",
    "ic(forward_function(x, a, b))\n",
    "\n",
    "y_pred_np = forward_function(x_np, a_np, b_np)\n",
    "y_pred = forward_function(x, a, b)\n",
    "loss = loss_function(y_pred, y_target)\n",
    "\n",
    "# Use autograd to compute the backward pass. This call will compute the\n",
    "# gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "# After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "# the gradient of the loss with respect to a, b, c, d respectively.\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ic(\" \")\n",
    "ic(\"backprop. of a (manually):\", (loss_backwards(y_pred_np, y_target_np) @ a_backwars(x_np, a_np, b_np)).item())\n",
    "ic(\"backprop. of a (with autograd):\", a.grad)\n",
    "ic(\" \")\n",
    "ic(\"backprop. of a (manually):\", (loss_backwards(y_pred_np, y_target_np) @ b_backwars(x_np, a_np, b_np)).item())\n",
    "ic(\"backprop. of b (with autograd):\", b.grad)\n",
    "ic(\" \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb873aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| ' '\n",
      "ic| \"backprop. of w (manually):\": 'backprop. of w (manually):'\n",
      "    loss_backwards(y_pred_np, y_target_np) @ w_backwars(x_np, w_np, b_np): array([[1.1763296 , 0.55752889],\n",
      "                                                                                  [1.36888214, 0.64879039],\n",
      "                                                                                  [0.36036128, 0.17079552]])\n",
      "ic| \"backprop. of w (with autograd):\": 'backprop. of w (with autograd):'\n",
      "    w.grad: tensor([[1.1763, 0.5575],\n",
      "                    [1.3689, 0.6488],\n",
      "                    [0.3604, 0.1708]])\n",
      "ic| ' '\n",
      "ic| \"backprop. of b (manually):\": 'backprop. of b (manually):'\n",
      "    loss_backwards(y_pred_np, y_target_np) @ b_backwars(x_np, w_np, b_np): array([1.2963876 , 1.50859235, 0.39714031])\n",
      "ic| \"backprop. of b (with autograd):\": 'backprop. of b (with autograd):'\n",
      "    b.grad: tensor([1.2964, 1.5086, 0.3971])\n",
      "ic| ' '\n",
      "ic| \"backprop. of b (manually):\": 'backprop. of b (manually):'\n",
      "    loss_backwards(y_pred_np, y_target_np) @ x_backwards(x_np, w_np, b_np): array([0.90048636, 1.76421675])\n",
      "ic| \"backprop. of b (with autograd):\": 'backprop. of b (with autograd):'\n",
      "    x.grad: tensor([0.9005, 1.7642])\n",
      "ic| ' '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward_function\n",
    "def forward_function(x, w, b):\n",
    "    # y1 = b1+w(1,1)*x1 + w(1,2)*x2\n",
    "    # y2 = b2+w(2,1)*x1 + w(2,2)*x2\n",
    "     return w @ x + b\n",
    "    \n",
    "# (d forward_function)/(d w) = (1, 1, ...)\n",
    "#def w_backwars(x, w, b):\n",
    "#     return torch.ones(len(x), dtype = dtype)\n",
    "    \n",
    "# (d forward_function)/(d b) = (x1, x2, ...)\n",
    "def b_backwars(x, w, b):\n",
    "    #(d y)/(d b) = ((d y1)/(d b), (d y2)/(d b)) \n",
    "    # = (((d y1)/(d b1), (d y1)/(d b2)), ((d y2)/(d b1), (d y2)/(d b2))) #Jacobimatrix\n",
    "    # = ((1, 0), (0, 1))\n",
    "    return np.eye(len(b)) # note len(y) = len(b) = # rows of w, because y = w*x +b\n",
    "\n",
    "def w_backwars(x, w, b):\n",
    "    # calculation for dimensions: w - 2x2, b - 2, x -2\n",
    "    # (d y)/(d w) = ((d y1)/(d w), (d y2)/(d w)) \n",
    "    # = ((((d y1)/(d w(1,1)), (d y1)/(d w(1,2))), ((d y1)/(d w(2,1)), (d y1)/(d w(2,2)))), (((d y2)/(d w(1,1)), (d y2)/(d w(1,2))), ((d y2)/(d w(2,1)), (d y2)/(d w(2,2))))) #Von uns veralgemeinerte Jacobimatrix (Tensor 3.stufe)\n",
    "    # = ((( x1              ,  x2              ), ( 0               ,  0               )), (( 0               ,  0               ), ( x1              ,  x2              )))\n",
    "    # = ([(x1, x2), (0,  0)], [(0,  0), (x1,  x2)])\n",
    "    # formaly: we have a function: R⁴ -> R², our Jacobian would be 4 x 2 matrix (the most inner dimension would\n",
    "    # be flatten out): J = ([(x1, x2, 0,  0)], [(0,  0, x1,  x2)]), with this the standard mathmatics works and the matrix products are defined\n",
    "    # but to keep dimensions of the matrix, we have the extra dimensions and the multiplication works, because the implentation \n",
    "    # of the matrix multiplication in pytorch and numpy cover the case of 1d-matrix @ 3d-matrix, and it does what we want, and it saves us from reshaping later \n",
    "    c = np.zeros((len(w), len(w), len(x)))\n",
    "    #= ((((d y1)/(d w(1,1)), (d y1)/(d w(1,2))), ((d y1)/(d w(2,1)), (d y1)/(d w(2,2)))), (((d y2)/(d w(1,1)), (d y2)/(d w(1,2))), ((d y2)/(d w(2,1)), (d y2)/(d w(2,2)))))\n",
    "    #= ([(x1, x2), (0,  0), (0, 0)], [(0,  0), (x1,  x2), (0, 0)], [(0,  0), (0,  0), (x1, x2)] )\n",
    "    for i in range(len(w)):\n",
    "        c[i,i] = x\n",
    "    return c\n",
    "    #return x.repeat(len(x),1) # does not work\n",
    "    \n",
    "def x_backwards(x, w, b):\n",
    "    #(d y)/(d x) = ((d y1)/(d x), (d y2)/(d x)) \n",
    "    # = (((d y1)/(d x1), (d y1)/(d x2)), ((d y2)/(d x1), (d y2)/(d x2)))\n",
    "    # = (( w(1,1)      ,  w(1,2)      ), ( w(2,1),        w(2,2)      )) = w\n",
    "    return w\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#input\n",
    "x_np = np.random.rand(2)\n",
    "x = torch.tensor(x_np, dtype = dtype, requires_grad=True)\n",
    "\n",
    "#target\n",
    "y_target_np = np.random.rand(3)\n",
    "y_target = torch.tensor(y_target_np, dtype = dtype)\n",
    "\n",
    "#parameters\n",
    "w_np = np.random.rand(3,2)\n",
    "w = torch.tensor(w_np, dtype=dtype, requires_grad=True)\n",
    "b_np = np.random.rand(3)\n",
    "b = torch.tensor(b_np, dtype = dtype, requires_grad=True)\n",
    "\n",
    "    \n",
    "y_pred = forward_function(x, w, b)\n",
    "y_pred_np = forward_function(x_np, w_np, b_np)\n",
    "loss = loss_function(y_pred, y_target)\n",
    "\n",
    "# Use autograd to compute the backward pass. This call will compute the\n",
    "# gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "# After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "# the gradient of the loss with respect to a, b, c, d respectively.\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "ic(\" \")\n",
    "ic(\"backprop. of w (manually):\", (loss_backwards(y_pred_np, y_target_np) @ w_backwars(x_np, w_np, b_np)))\n",
    "ic(\"backprop. of w (with autograd):\", w.grad)\n",
    "ic(\" \")  \n",
    "ic(\"backprop. of b (manually):\", (loss_backwards(y_pred_np, y_target_np) @ b_backwars(x_np, w_np, b_np)))\n",
    "ic(\"backprop. of b (with autograd):\", b.grad)\n",
    "ic(\" \")\n",
    "ic(\"backprop. of b (manually):\", (loss_backwards(y_pred_np, y_target_np) @ x_backwards(x_np, w_np, b_np)))\n",
    "ic(\"backprop. of b (with autograd):\", x.grad)\n",
    "ic(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d47cbcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| np.array((2)).ndim: 0\n",
      "ic| np.float64(2).ndim: 0\n",
      "ic| np.array([x]): array([[1, 2]])\n",
      "ic| np.diag(np.diag(z)): array([[1]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]] [1 2]\n",
      "[0. 0. 0.]\n",
      "6\n",
      "3\n",
      "[2 4]\n",
      "[6 6]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All dimensions of input must be of equal length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m ic(np\u001b[38;5;241m.\u001b[39mdiag(np\u001b[38;5;241m.\u001b[39mdiag(z)))\n\u001b[1;32m     18\u001b[0m c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(w), \u001b[38;5;28mlen\u001b[39m(w), \u001b[38;5;28mlen\u001b[39m(x)))\n\u001b[0;32m---> 19\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_diagonal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyter_venv/.venv/lib/python3.12/site-packages/numpy/lib/_index_tricks_impl.py:942\u001b[0m, in \u001b[0;36mfill_diagonal\u001b[0;34m(a, val, wrap)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;66;03m# For more than d=2, the strided formula is only valid for arrays with\u001b[39;00m\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;66;03m# all dimensions equal, so we check first.\u001b[39;00m\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(diff(a\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 942\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll dimensions of input must be of equal length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    943\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (np\u001b[38;5;241m.\u001b[39mcumprod(a\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# Write the value out into the diagonal.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: All dimensions of input must be of equal length"
     ]
    }
   ],
   "source": [
    "# scratch:\n",
    "c = np.zeros((3, 2))\n",
    "x = np.array([1,2])\n",
    "print(c,x)\n",
    "print(c@x)\n",
    "print(c.size)\n",
    "print(len(c))\n",
    "print(np.add(x,x))\n",
    "\n",
    "print(np.array((2))*np.array((3,3)))\n",
    "np.array((2,3))@np.array((3,3))\n",
    "\n",
    "ic(np.array((2)).ndim)\n",
    "ic(np.float64(2).ndim)\n",
    "z = ic(np.array([x]))\n",
    "\n",
    "ic(np.diag(np.diag(z)))\n",
    "c = np.zeros((len(w), len(w), len(x)))\n",
    "np.fill_diagonal(c[:,:],x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0f22ecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]] [1 2]\n",
      "[0. 0. 0.]\n",
      "6\n",
      "3\n",
      "[2 4]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd775bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
