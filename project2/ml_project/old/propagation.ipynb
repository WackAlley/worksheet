{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8668e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expr_node' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ABC, abstractmethod\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mfunction\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mABC\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;129;43m@property\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;129;43m@abstractmethod\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mforward_func\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mCallable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m, in \u001b[0;36mfunction\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_functions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Callable, Callable]:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_func\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minner) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mexpr_node\u001b[49m: \u001b[38;5;66;03m# simplifies the syntax for chaning of functions\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m expr_node(\u001b[38;5;28mself\u001b[39m, inner)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'expr_node' is not defined"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from typing import Callable\n",
    "from typing import Type\n",
    "from typing import Tuple\n",
    "#from collections.abc import Iterable \n",
    "#@abstractmethod\n",
    "#    @property\n",
    "#    @abstractmethod\n",
    "# no need to call super\n",
    "from icecream import ic\n",
    "    \n",
    "    \n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "        \n",
    "class function(ABC):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def forward_func(self) -> Callable:\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def backward_func(self) -> Callable:\n",
    "        pass\n",
    "    \n",
    "    def get_functions(self) -> Tuple[Callable, Callable]:\n",
    "        return self.forward_func, self.backward_func\n",
    "    \n",
    "    def __call__(self, *inner) -> expr_node: # simplifies the syntax for chaning of functions\n",
    "        return expr_node(self, inner)\n",
    "        \n",
    "    \n",
    "\n",
    "class sin(function):\n",
    "    forward_func = np.sin\n",
    "    backward_func = np.cos\n",
    "\n",
    "class tanh(function):\n",
    "    forward_func = np.tanh\n",
    "    backward_func = lambda x: 1 - np.square(tanh(x))\n",
    "\n",
    "class cos(function):\n",
    "    forward_func = np.cos\n",
    "    backward_func = lambda x: -np.sin(x)\n",
    "          \n",
    "\n",
    "class multiply(function):\n",
    "    \n",
    "    forward_func = lambda: None # to be overwitten in constructor\n",
    "    backward_func = lambda: None # to be overwitten in constructor\n",
    "    \n",
    "    def __init__(self,*, allow_arbitrary_many = False): # * makes allow_arbitary_many to keword only\n",
    "        if not allow_arbitrary_many: # simple case two factors\n",
    "            self.forward_func = np.multiply\n",
    "            self.backward_func = lambda x, y: (y, x)\n",
    "        else:\n",
    "            forward_func = lambda *x: np.prod(np.vstack(x), axis=0)\n",
    "            self.backward_func = lambda *x: (np.prod(np.vstack(values), axis=0) for values in combinations(x, len(x) - 1))\n",
    "                   \n",
    "            \n",
    "        \n",
    "class add(function):\n",
    "    \n",
    "    forward_func = lambda: None # to be overwitten in constructor\n",
    "    backward_func = lambda: None # to be overwitten in constructor\n",
    "    \n",
    "    def __init__(self,*, allow_arbitrary_many = False): # new is clalled before the constructer (before: \"__init__\")\n",
    "        if not allow_arbitrary_many:\n",
    "            self.forward_func = np.add \n",
    "            self.backward_func = lambda x, y: (np.full_like(x, 1), np.full_like(y, 1)) #arrays with same shape of x and y, filled with 1\n",
    "        else:\n",
    "            self.forward_func = lambda *x: np.sum(np.vstack(x), axis=0)\n",
    "            self.backward_func = lambda *x: (np.full_like(x[0], 1) for _ in range(len(x))) #backward_func = lambda *x: (np.ones(len(x[0]) if isinstance(x[0], Iterable) else 1) for _ in range(len(x))))\n",
    "\n",
    "            \n",
    "                    \n",
    "            \n",
    "class matrix_vector_product(function):\n",
    "    def matrix_vector_backwards(self, matrix, vector):\n",
    "        c = np.zeros((len(matrix), len(matrix), len(vector)))\n",
    "        for i in range(len(matrix)):\n",
    "            c[i,i] = vector\n",
    "        return (c, matrix) \n",
    "    forward_func = np.matmul\n",
    "    backward_func = matrix_vector_backwards\n",
    "    \n",
    "class vector_vector_sum(function):\n",
    "    #vectors must have same length\n",
    "    forward_func = np.add\n",
    "    backward_func = lambda v1, v2: (np.eye(len(v1)), np.eye(len(v2)))\n",
    "    \n",
    "    \n",
    "    \n",
    "#b = function2(np.sin, np.cos)\n",
    "c = sin()\n",
    "print(c, c.forward_func, c.backward_func)\n",
    "f1, f2 = c.get_functions()\n",
    "print(f1, f2)\n",
    "#c.eval_forward(2)\n",
    "d = [sin(), sin()]\n",
    "\n",
    "\n",
    "e = multiply()\n",
    "print(e.forward_func)\n",
    "\n",
    "\n",
    "e = multiply(allow_arbitrary_many=True)\n",
    "print(e.forward_func)\n",
    "e = multiply(allow_arbitrary_many=False)\n",
    "print(e.forward_func)\n",
    "f = cos()\n",
    "print(f.forward_func(2), np.cos(2))  \n",
    "forw, backw = c.get_functions()\n",
    "print(forw, backw) \n",
    "ic.disable()\n",
    "ghj = ic(f.forward_func(2))\n",
    "ic(ghj)\n",
    "k = tanh()\n",
    "l = cos()\n",
    "ic.enable()\n",
    "ic(l.forward_func(2))\n",
    "ic()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "83350876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from __future__ import annotations\n",
    "from numbers import Number\n",
    "\n",
    "class expr_end_node():\n",
    "    def __init__(self, value: Number | np.ndarray , grad_value: Number | np.ndarray  = 0):\n",
    "        self.value = value\n",
    "        self.grad_value = grad_value\n",
    "        \n",
    "class expr_node():\n",
    "    def __init__(self, func: function, childs: Sequence[expr_node | expr_end_node] = []):\n",
    "        #self.parants = parants #expr_node\n",
    "        self.childs = childs  #expr_node\n",
    "        self.func = func #function \n",
    "        # lieber so:\n",
    "        # self.forward_func, self.backward_func = func.get_functions()\n",
    "        # und dann self.func entfernen\n",
    "        \n",
    "f = sin()\n",
    "g = sin()\n",
    "ex1 = expr_node(f)\n",
    "ex2 = expr_node(g)\n",
    "ex3 = expr_node(g)\n",
    "ex1.childs = [ex2]\n",
    "#ex2.parants = [ex1]\n",
    "ex2.childs = [ex3]\n",
    "#sin(sin(sin()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ee800e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.expr_node at 0x72b559964620>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex1 = expr_node(sin())\n",
    "ex2 = expr_node(sin())\n",
    "ex3 = expr_node(sin())\n",
    "x = expr_end_node(np.float64(0.345))\n",
    "ex1.childs = [ex2]\n",
    "ex2.childs = [ex3]\n",
    "ex3.childs = [x]\n",
    "\n",
    "#sin(sin(sin(x)))\n",
    "\"\"\"\n",
    "sin\n",
    " |\n",
    "sin\n",
    " |\n",
    "sin\n",
    " |\n",
    " x=0.345\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#cos(a)*sin(b + tanh(c))\n",
    "example_expr2 = expr_node(multiply(), \n",
    "                          [expr_node(cos(),\n",
    "                                     [expr_end_node(np.float64(0.345))]\n",
    "                                    ), \n",
    "                           expr_node(sin(),\n",
    "                                     [expr_node(add(), \n",
    "                                                [expr_end_node(np.float64(0.345)),\n",
    "                                                 expr_node(tanh(), \n",
    "                                                           [expr_end_node(np.float64(0.345))]\n",
    "                                                          )\n",
    "                                                ]\n",
    "                                               )\n",
    "                                     ]\n",
    "                                    )\n",
    "                          ]\n",
    "                         )\n",
    "\n",
    "# the same using the __call__ method of function (see above in function definition):\n",
    "example_expr3 = multiply()(\n",
    "                        cos()(\n",
    "                            expr_end_node(np.float64(0.345))\n",
    "                        ),\n",
    "                        sin()(\n",
    "                            add()(\n",
    "                                expr_end_node(np.float64(0.345)),\n",
    "                                tanh()(\n",
    "                                    expr_end_node(np.float64(0.345))\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                )\n",
    "\"\"\"\n",
    "    multiply\n",
    "    |      |\n",
    "   cos    sin\n",
    "   |       |\n",
    "  a=0.345 add\n",
    "          | |\n",
    "    b=0.345 tanh\n",
    "             |\n",
    "         c=0.345\n",
    "\"\"\"\n",
    "#short version of the sin(sin(sin(x))) exaple:\n",
    "x = expr_end_node(np.float64(0.345))\n",
    "example_expr1 = sin()( sin()( sin()( x ) ) ) \n",
    "\n",
    "x1 = expr_end_node(np.float64(0.345))\n",
    "x2 = expr_end_node(np.float64(0.345))\n",
    "x3 = expr_end_node(np.float64(0.345))\n",
    "x4 = expr_end_node(np.float64(0.345))\n",
    "sin()(multiply())\n",
    "#sin(x1*x2*cos(x3*x4))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e3932d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph_out/tt/the_graph.gv.pdf'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!source .jupyter_venv/.venv/bin/activate # source venv, if you have one for your kernal\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install graphviz\n",
    "\n",
    "import graphviz\n",
    "\n",
    "\n",
    "def print_graph(node, graph, parent_id=\"\"):\n",
    "    \n",
    "    node_id = str(id(node))\n",
    "    node_label = str(node.value) if isinstance(node, expr_end_node) else type(node.func).__name__\n",
    "    graph.node(node_id, node_label)  \n",
    "    \n",
    "    if parent_id:\n",
    "        graph.edge(parent_id, node_id)\n",
    "        \n",
    "    if hasattr(node, 'childs') and node.childs:\n",
    "            for child in node.childs:\n",
    "                print_graph(child, graph, node_id)#, constraint='false')\n",
    "\n",
    "\n",
    "dot1 = graphviz.Digraph('the_graph', comment='test') \n",
    "dot1.attr(rankdir=\"LR\")\n",
    "print_graph(example_expr3, dot1)\n",
    "#doctest_mark_exe() \n",
    "dot1.render(directory='graph_out/tt', view=True)\n",
    "#dot  \n",
    "#str(id(ex1))\n",
    "#type(ex1.func).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ee0e80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ex1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(node):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39mforward_func(\u001b[38;5;241m*\u001b[39m(forward(child) \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchilds)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(node) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m expr_end_node \u001b[38;5;28;01melse\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue \n\u001b[0;32m----> 7\u001b[0m forward(\u001b[43mex1\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mdef forward(node):\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    if type(node) is not expr_end_node:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m        node.value \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m forward(ex1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ex1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#f(g(h(x)))' = f'(g(h(x))) * (g(h(x)))' =  f'(g(h(x))) * g'(h(x)) * h'(x)\n",
    "# h'(x) * g'(h(x)) * f'(g(h(x)))\n",
    "\n",
    "\n",
    "def forward(node):\n",
    "    return node.func.forward_func(*(forward(child) for child in node.childs)) if type(node) is not expr_end_node else node.value \n",
    "forward(ex1)\n",
    "\"\"\"\n",
    "def forward(node):\n",
    "    if type(node) is not expr_end_node:\n",
    "    \n",
    "        node.func.forward_func(*(forward(child) for child in node.childs))\n",
    "    else:\n",
    "        node.value \n",
    "\"\"\"\n",
    "forward(ex1)\n",
    "\n",
    "def backward(node, value = 1):\n",
    "    if type(node) is not expr_end_node:\n",
    "        child_values = [forward(child) for child in node.childs]\n",
    "        if len(node.childs) == 1:\n",
    "            backward(node.childs[0], value * node.func.backward_func(*child_values))\n",
    "        else:\n",
    "            for child, new_value in zip(node.childs, node.func.backward_func(*child_values), strict=True):\n",
    "                backward(child, value * new_value)\n",
    "    else: \n",
    "        node.grad_value += value\n",
    "        \n",
    "#result,target\n",
    "    \n",
    "        \n",
    "        \n",
    "x.grad_value=0\n",
    "\n",
    "ic(forward(ex1))\n",
    "ic(np.sin(np.sin(np.sin(np.float64(.345)))))\n",
    "\n",
    "backward(ex1)\n",
    "ic(x.grad_value)\n",
    "ic(np.cos(np.sin(np.sin(.345)))*np.cos(np.sin(.345))*np.cos(0.345))\n",
    "\n",
    "print(x.grad_value)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def print_all(a,b,c):\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)\n",
    "\n",
    "print_all(*[1,2,3])\n",
    "\"\"\"\n",
    "#zip((a1,a2.a3),(b1,b2,b3) -> ((a1,b1),(a2,b2),(a3,b3))\n",
    "\n",
    "x1 = expr_end_node(np.float64(3.11))\n",
    "x2 = expr_end_node(np.float64(1.11))\n",
    "b = multiply()(x1,x2)\n",
    "ic(forward(b))\n",
    "backward(b)\n",
    "ic(x1.grad_value, x2.grad_value)\n",
    "\n",
    "mat = expr_end_node(np.random.rand(3,2))\n",
    "vec = expr_end_node(np.random.rand(2))\n",
    "a = matrix_vector_product()(mat, vec)\n",
    "forward(a)\n",
    "backward(a)\n",
    "ic(mat.value, vec.value)\n",
    "ic(mat.grad_value, vec.grad_value )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2dff8a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| iris.target: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "                        2, 2, 2, 2, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 150\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "                        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "ic| LA.norm(iris.data, axis=0): array([72.27620632, 37.82062929, 50.82036993, 17.38763929])\n",
      "ic| LA.norm(iris.data,np.inf, axis=0): array([7.9, 4.4, 6.9, 2.5])\n",
      "ic| np.max(iris.data, axis=0): array([7.9, 4.4, 6.9, 2.5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nout = vector len 3 , out_1: suit 1,  out_1: suit 2, \\nsuit_1 = (1,0,0)\\nsuit_2 = (0,1,0)\\nsuit_3 = (0,0,1)\\n\\nnetz = \\n    in: vecor len 4 \\n    out = vector len 3\\n    \\nwith  2-5 hidden layer; 4- 10 neurons each\\n'"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from numpy import linalg as LA\n",
    "\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "print(len(iris.data), len(iris.target))\n",
    "print(type(iris.data), type(iris.target))\n",
    "ic(iris.target)\n",
    "normalized_data = iris.data/LA.norm(iris.data, axis=0)\n",
    "\n",
    "#%timeit LA.norm(iris.data, axis=0)\n",
    "#%timeit LA.norm(iris.data,np.inf, axis=0)\n",
    "#%timeit np.max(iris.data, axis=0)\n",
    "\n",
    "ic(LA.norm(iris.data, axis=0))\n",
    "ic(LA.norm(iris.data,np.inf, axis=0))\n",
    "ic(np.max(iris.data, axis=0))\n",
    "\"\"\"\n",
    "out = vector len 3 , out_1: suit 1,  out_1: suit 2, \n",
    "suit_1 = (1,0,0)\n",
    "suit_2 = (0,1,0)\n",
    "suit_3 = (0,0,1)\n",
    "\n",
    "netz = \n",
    "    in: vecor len 4 \n",
    "    out = vector len 3\n",
    "    \n",
    "with  2-5 hidden layer; 4- 10 neurons each\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e956fc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| loss_mse(test_prediction, test_target): np.float64(0.29000000000000004)\n",
      "ic| np.log(test_prediction): array([-1.60943791, -0.51082562, -1.2039728 ])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.60943791, -0.51082562, -1.2039728 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction = np.array([0.2, 0.6, 0.3])\n",
    "test_target = np.array([0, 1, 0])\n",
    "\n",
    "# Mean squared error (MSE):\n",
    "def loss_mse(prediction, target):\n",
    "    return np.square(prediction - target).sum()\n",
    "#(pred_x, pred_y), (target_x, target_y) -> (pred_x - target_x)²+(pred_y - target_y)²\n",
    "\n",
    "\n",
    "\n",
    "loss1 = ic(loss_mse(test_prediction, test_target))\n",
    "ic(np.log(test_prediction))\n",
    "#loss2 = ic(loss_ce(test_prediction,test_target))\n",
    "\n",
    "#ic.disable()\n",
    "#ic.enable()\n",
    "\n",
    "# 0.2²+0.4²+0.3² = \n",
    "#ic(loss1(test_prediction, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a782488b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1707078846.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    in_1     out_1\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "in_1     out_1\n",
    "in_2     out_2\n",
    "\n",
    "initial = w, b are random values\n",
    "netz(in) = \n",
    "    out_1 = in_1 * w_{1,1} + in_2 *w_{1,2} + b_1\n",
    "    out_2 = in_1 * w_{2,1} + in_2 *w_{2,2} + b_2\n",
    "\n",
    "# net(in, {w}, {b}) \n",
    "out = netz(in)\n",
    "loss(out, true_val)\n",
    "backpropergation -> change  w, b <--------\n",
    "\n",
    "\n",
    "\n",
    "dict(np.sin: np.cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c8711c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prin_list(a, b, *x):\n",
    "        print(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b402a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prin_list(1,25555555555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8226a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ada42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1041623",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b0d27ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: Zeile 1: .jupyter_venv/.venv/bin/activate: Datei oder Verzeichnis nicht gefunden\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.5.1%2Bcpu-cp312-cp312-linux_x86_64.whl (174.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.6/174.6 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.20.1%2Bcpu-cp312-cp312-linux_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.5.1%2Bcpu-cp312-cp312-linux_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/setuptools-70.0.0-py3-none-any.whl (863 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: numpy in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, setuptools, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.2.0 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 setuptools-70.0.0 sympy-1.13.1 torch-2.5.1+cpu torchaudio-2.5.1+cpu torchvision-0.20.1+cpu typing-extensions-4.9.0\n"
     ]
    }
   ],
   "source": [
    "!source .jupyter_venv/.venv/bin/activate # source venv, if you have one for your kernal\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99ca43d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 815.4979248046875\n",
      "199 579.5009155273438\n",
      "299 412.5529479980469\n",
      "399 294.4494934082031\n",
      "499 210.89791870117188\n",
      "599 151.7886199951172\n",
      "699 109.97039794921875\n",
      "799 80.38469696044922\n",
      "899 59.45298767089844\n",
      "999 44.64371871948242\n",
      "1099 34.16594314575195\n",
      "1199 26.752681732177734\n",
      "1299 21.50754165649414\n",
      "1399 17.796405792236328\n",
      "1499 15.170598983764648\n",
      "1599 13.312688827514648\n",
      "1699 11.998108863830566\n",
      "1799 11.067950248718262\n",
      "1899 10.409802436828613\n",
      "1999 9.944101333618164\n",
      "Result: y = 0.03551137074828148 + 0.8578885197639465 x + -0.006126299500465393 x^2 + -0.09349364042282104 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7840bc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| forward_function(x, a, b): tensor([-0.7378, -1.2779], grad_fn=<AddBackward0>)\n",
      "ic| ' '\n",
      "ic| \"backprop. of a (manually):\": 'backprop. of a (manually):'\n",
      "    (loss_backwards(y_pred, y_target)@a_backwars(x, a, b)).item(): -6.031282424926758\n",
      "ic| 'backprop. of a (with autograd):', a.grad: tensor(-6.0313)\n",
      "ic| ' '\n",
      "ic| \"backprop. of a (manually):\": 'backprop. of a (manually):'\n",
      "    (torch.t(a_backwars(x, a, b))@loss_backwards(y_pred, y_target)).item(): -6.031282424926758\n",
      "ic| 'backprop. of a (with autograd):', a.grad: tensor(-6.0313)\n",
      "ic| ' '\n",
      "ic| \"backprop. of b (manually):\": 'backprop. of b (manually):'\n",
      "    (loss_backwards(y_pred, y_target)@b_backwars(x, a, b)).item(): -2.5557620525360107\n",
      "ic| 'backprop. of b (with autograd):', b.grad: tensor(-2.5558)\n",
      "ic| ' '\n",
      "ic| \"backprop. of b (manually):\": 'backprop. of b (manually):'\n",
      "    (torch.t(b_backwars(x, a, b))@loss_backwards(y_pred, y_target)).item(): -2.5557620525360107\n",
      "ic| 'backprop. of b (with autograd):', b.grad: tensor(-2.5558)\n",
      "ic| b.grad: tensor(-2.5558)\n",
      "ic| ' '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "#input\n",
    "x = torch.tensor([0, 1], dtype = dtype)\n",
    "#target\n",
    "y_target = torch.tensor([1, 0], dtype = dtype)\n",
    "\n",
    "#parameters\n",
    "a = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "\n",
    "# forward_function\n",
    "def forward_function(x, a, b):\n",
    "    # y1 = a+b*x1\n",
    "    # y2 = a+b*x2\n",
    "    # ....\n",
    "     return a + b * x\n",
    "    \n",
    "# (d forward_function)/(d a) = (1, 1, ...)\n",
    "def a_backwars(x, a, b):\n",
    "     return torch.ones(x.shape, dtype = dtype)\n",
    "     #return torch.tensor([1 for _ in range(len(forward_function(x, a, b)))], dtype = dtype)\n",
    "    \n",
    "# (d forward_function)/(d b) = (x1, x2, ...)\n",
    "def b_backwars(x, a, b):\n",
    "     return x\n",
    "    \n",
    "# loss_function   \n",
    "def loss_function(y_pred, y_target):\n",
    "     return (y_pred - y_target).pow(2).sum()\n",
    "    \n",
    "# (d loss)/(d y_pred) = 2*(y_pred - y_target)\n",
    "def loss_backwards(y_pred, y_target):\n",
    "     return 2*(y_pred - y_target)\n",
    "    \n",
    "ic(forward_function(x, a, b))\n",
    "y_pred = forward_function(x, a, b)\n",
    "loss = loss_function(y_pred, y_target)\n",
    "\n",
    "# Use autograd to compute the backward pass. This call will compute the\n",
    "# gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "# After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "# the gradient of the loss with respect to a, b, c, d respectively.\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ic(\" \")\n",
    "ic(\"backprop. of a (manually):\", (loss_backwards(y_pred, y_target)@a_backwars(x, a, b)).item())\n",
    "ic(\"backprop. of a (with autograd):\", a.grad)\n",
    "ic(\" \")\n",
    "ic(\"backprop. of a (manually):\", (torch.t(a_backwars(x, a, b))@loss_backwards(y_pred, y_target)).item())\n",
    "ic(\"backprop. of a (with autograd):\", a.grad)\n",
    "ic(\" \")\n",
    "ic(\"backprop. of b (manually):\", (loss_backwards(y_pred, y_target)@b_backwars(x, a, b)).item())\n",
    "ic(\"backprop. of b (with autograd):\", b.grad)\n",
    "ic(\" \")\n",
    "ic(\"backprop. of b (manually):\", (torch.t(b_backwars(x, a, b))@loss_backwards(y_pred, y_target)).item())\n",
    "ic(\"backprop. of b (with autograd):\", b.grad)\n",
    "ic(b.grad)\n",
    "ic(\" \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "812cdc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| loss_backwards(y_pred, y_target): tensor([-0.6472,  1.5129], grad_fn=<MulBackward0>)\n",
      "    w_backwars(x, w, b): tensor([[[-0.3763,  0.3583],\n",
      "                                  [ 0.0000,  0.0000]],\n",
      "                         \n",
      "                                 [[ 0.0000,  0.0000],\n",
      "                                  [-0.3763,  0.3583]]], grad_fn=<CopySlices>)\n",
      "ic| test: tensor([[ 0.2435, -0.2319],\n",
      "                  [-0.5693,  0.5420]], grad_fn=<AddBackward0>)\n",
      "ic| ' '\n",
      "ic| \"backprop. of w (manually):\": 'backprop. of w (manually):'\n",
      "    loss_backwards(y_pred, y_target)@w_backwars(x, w, b): tensor([[ 0.2435, -0.2319],\n",
      "                                                                  [-0.5693,  0.5420]], grad_fn=<UnsafeViewBackward0>)\n",
      "ic| \"backprop. of w (with autograd):\": 'backprop. of w (with autograd):'\n",
      "    w.grad: tensor([[ 0.2435, -0.2319],\n",
      "                    [-0.5693,  0.5420]])\n",
      "ic| ' '\n",
      "ic| \"backprop. of b (manually):\": 'backprop. of b (manually):'\n",
      "    loss_backwards(y_pred, y_target)@b_backwars(x, w, b): tensor([-0.6472,  1.5129], grad_fn=<SqueezeBackward4>)\n",
      "ic| \"backprop. of b (with autograd):\": 'backprop. of b (with autograd):'\n",
      "    b.grad: tensor([-0.6472,  1.5129])\n",
      "ic| ' '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dtype = torch.float\n",
    "\n",
    "#input\n",
    "#x = torch.tensor([0, 1], dtype = dtype)\n",
    "x = torch.randn((2), dtype=dtype, requires_grad=True)\n",
    "\n",
    "#target\n",
    "#y_target = torch.tensor([1, 0], dtype = dtype)\n",
    "y = torch.randn((2), dtype=dtype, requires_grad=True)\n",
    "\n",
    "#parameters\n",
    "#w = torch.arange(4).reshape(2,2)\n",
    "#w = torch.tensor(w.clone(), dtype=dtype, requires_grad=True)\n",
    "#b = torch.arange(2, dtype=dtype, requires_grad=True)\n",
    "\n",
    "w = torch.randn((2,2), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((2), dtype=dtype, requires_grad=True)\n",
    "\n",
    "# forward_function\n",
    "def forward_function(x, w, b):\n",
    "    # y1 = b1+w(1,1)*x1 + w(1,2)*x2\n",
    "    # y2 = b2+w(2,1)*x1 + w(2,2)*x2\n",
    "     return w @ x + b\n",
    "    \n",
    "# (d forward_function)/(d w) = (1, 1, ...)\n",
    "#def w_backwars(x, w, b):\n",
    "#     return torch.ones(len(x), dtype = dtype)\n",
    "    \n",
    "# (d forward_function)/(d b) = (x1, x2, ...)\n",
    "def b_backwars(x, w, b):\n",
    "    #(d y)/(d b) = ((d y1)/(d b), (d y2)/(d b)) \n",
    "    # = (((d y1)/(d b1), (d y1)/(d b2)), ((d y2)/(d b1), (d y2)/(d b2))) #Jacobimatrix\n",
    "    # = ((1, 0), (0, 1))\n",
    "    return torch.eye(len(x), dtype = dtype)\n",
    "\n",
    "def w_backwars(x, w, b):\n",
    "    # (d y)/(d w) = ((d y1)/(d w), (d y2)/(d w)) \n",
    "    # = ((((d y1)/(d w(1,1)), (d y1)/(d w(1,2))), ((d y1)/(d w(2,1)), (d y1)/(d w(2,2)))), (((d y2)/(d w(1,1)), (d y2)/(d w(1,2))), ((d y2)/(d w(2,1)), (d y2)/(d w(2,2))))) #Von uns veralgemeinerte Jacobimatrix (Tensor 3.stufe)\n",
    "    # = ((( x1              ,  x2              ), ( 0               ,  0               )), (( 0               ,  0               ), ( x1              ,  x2              )))\n",
    "    # = ([(x1, x2), (0,  0)], [(0,  0), (x1,  x2)])\n",
    "    # \n",
    "    c = torch.zeros(len(x), len(x), len(x), dtype=x.dtype) \n",
    "    for i in range(len(x)):\n",
    "        c[i,i] = x\n",
    "    return c\n",
    "    #return x.repeat(len(x),1) # does not work\n",
    "    \n",
    "    \n",
    "# loss_function   \n",
    "def loss_function(y_pred, y_target):\n",
    "     return (y_pred - y_target).pow(2).sum()\n",
    "    \n",
    "# (d loss)/(d y_pred) = 2*(y_pred - y_target)\n",
    "def loss_backwards(y_pred, y_target):\n",
    "     return 2*(y_pred - y_target)\n",
    "    \n",
    "y_pred = forward_function(x, w, b)\n",
    "loss = loss_function(y_pred, y_target)\n",
    "\n",
    "# Use autograd to compute the backward pass. This call will compute the\n",
    "# gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "# After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "# the gradient of the loss with respect to a, b, c, d respectively.\n",
    "loss.backward()\n",
    "\n",
    "ic(loss_backwards(y_pred, y_target),w_backwars(x, w, b) )\n",
    "test = w_backwars(x, w, b)[0] * loss_backwards(y_pred, y_target)[0] + w_backwars(x, w, b)[1] * loss_backwards(y_pred, y_target)[1]\n",
    "ic(test)\n",
    "ic(\" \")\n",
    "ic(\"backprop. of w (manually):\", (loss_backwards(y_pred, y_target)@w_backwars(x, w, b)))\n",
    "ic(\"backprop. of w (with autograd):\", w.grad)\n",
    "ic(\" \")\n",
    "ic(\"backprop. of b (manually):\", (loss_backwards(y_pred, y_target)@b_backwars(x, w, b)))\n",
    "ic(\"backprop. of b (with autograd):\", b.grad)\n",
    "#ic(b.grad)\n",
    "ic(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3ed75d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| a: tensor([[-1.9353, -1.2393],\n",
      "               [ 2.0114, -0.1456]], requires_grad=True)\n",
      "ic| b: tensor([ 0.8283, -0.0090], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8283, -0.0090], requires_grad=True)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((2,2), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((2), dtype=dtype, requires_grad=True)\n",
    "ic(a)\n",
    "ic(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4c718283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| b: tensor([[1., 0.],\n",
      "               [0., 1.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3]])\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1, 0, 0],\n",
      "        [0, 2, 0],\n",
      "        [0, 0, 3]])\n",
      "tensor([[1, 0, 0],\n",
      "        [0, 2, 0],\n",
      "        [0, 0, 3]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "\n",
    "b = torch.eye(2)\n",
    "ic(b)\n",
    "a = torch.tensor([1,2,3])\n",
    "c = torch.tensor([[[1, 2],[0, 0]], [[0, 0], [1, 2]]])\n",
    "#c = torch.stack([a.expand(2, 2), torch.diag(a)])\n",
    "c = torch.zeros(len(a), len(a), len(a), dtype=a.dtype) \n",
    "for i in range(len(a)):\n",
    "    c[i,i] = a\n",
    "\n",
    "c = a.repeat(len(a),1)\n",
    "\n",
    "print(c)\n",
    "#ic(torch.outer(b, a))\n",
    "print(a)\n",
    "print(torch.diag_embed(a))\n",
    "print(torch.diag(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65b0d134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyhafas in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (0.6.0)\n",
      "Requirement already satisfied: requests~=2.9 in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (from pyhafas) (2.32.3)\n",
      "Requirement already satisfied: pytz>=2013.6 in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (from pyhafas) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (from requests~=2.9->pyhafas) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (from requests~=2.9->pyhafas) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (from requests~=2.9->pyhafas) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (from requests~=2.9->pyhafas) (2024.8.30)\n",
      "Requirement already satisfied: pandas in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/marvin/.jupyter_venv/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!source .jupyter_venv/.venv/bin/activate # source venv, if you have one for your kernal\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install pyhafas\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "b86b7541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2\n"
     ]
    }
   ],
   "source": [
    "list = [i for i in range(3)]\n",
    "print(*list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b04d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_kernel",
   "language": "python",
   "name": "main_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
