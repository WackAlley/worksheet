{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07d7a675-fc31-4a1a-a56f-f7128802c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f6ed9-4711-4b53-8ec4-b5b6bf682edd",
   "metadata": {},
   "source": [
    "# Modules\n",
    "- To better organize our project, we outsourced certain components that support the main code presented in this notebook.\n",
    "\n",
    "### Atomic Operations, and Expression Rree\n",
    "\n",
    "- Each atomic operation is implemented as a separate class.  \n",
    "- Mathematical functions can be built by chaining these atomic operations together.  \n",
    "- These functions are internally represented using an *expression tree*.  \n",
    "  - In the expression tree:  \n",
    "    - Each atomic operation is a node, more specifically an instance of $\\texttt{Expr\\_node()}$.  \n",
    "    - Each variable in the function is also represented as a node, more specifically an instance of $\\texttt{Expr\\_node()}$. \n",
    " - the classes of atomic operation all inharit frome the same abstract classes, which defines their sturcture:\n",
    "     - all operations have a `forward` and a `backward` function, where the backward function is the derivative of the forward function \n",
    "      - These classes are callable. Calling an operation object on another value automatically builds the *expression tree*.  \n",
    "  - To calculate the `forward pass`, the expression tree is traversed using the `forward` function.  \n",
    "  - The `backward pass` requires both the `forward` and `backward` functions to compute derivatives, applying the **chain rule** during traversal.\n",
    "  \n",
    "### Visualize the Expression Tree\n",
    "- $\\texttt{print\\_graph}$: module provides everything needed to visualize the expression tree using the **Graphviz** package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0e12b73-3264-48c8-abe4-a9d3dab1702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forward_backward_functions_and_nodes import * \n",
    "from print_graph import print_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af883d-fb84-4c7d-9503-d86bfd890234",
   "metadata": {},
   "source": [
    "#  Details on our Forward and Backward Propagation Algorithms\n",
    "Both the forward and the backward pass are recursive functions.\n",
    "The forward pass computes the value of the specified node, this is the evaluation of the function at the points (values) of the end nodes. If the node is not an endnode, it will keep recurring itself until it reaches one. \\\n",
    "The backward propagation calculates the outer derivative at the current node by evaluating the backward function at the values from the forward pass of all child nodes and recursively calls itself on each child node. In each recursion step, the derivative values are multiplied. The propagation continues until it reaches an end node, where the accumulated product is assigned to the value of the end node.\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "054932a3-ff42-4ef3-bad1-7591b1f0e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(node):\n",
    "    return node.forward_func(*(forward(child) for child in node.childs)) if type(node) is not Expr_end_node else node.value \n",
    "    \n",
    "# def backward(node, value = np.float64(1)):\n",
    "#     if type(node) is not Expr_end_node:\n",
    "#         child_values = [forward(child) for child in node.childs] # computes the argument of the outer derivative. In other words: it computes g of f'(g)\n",
    "#         if len(node.childs) == 1:\n",
    "#             new_value = node.backward_func(*child_values) # computes the outer derivative f'(g)\n",
    "#             if value.ndim == 0 or new_value.ndim == 0:\n",
    "#                 backward(node.childs[0], value * new_value)\n",
    "#             else: \n",
    "#                 backward(node.childs[0], new_value.T @ value) # @ is matrix product\n",
    "#         else:\n",
    "#             for child, new_value in zip(node.childs, node.backward_func(*child_values), strict=True):\n",
    "#                 if value.ndim == 0 or new_value.ndim == 0:\n",
    "#                     backward(child, value * new_value)\n",
    "#                 else: \n",
    "#                     backward(child, new_value.T @ value)                 \n",
    "#     else:\n",
    "#         node.grad_value += value\n",
    "\n",
    "\n",
    "def backward(node, value = np.float64(1)):\n",
    "    if type(node) is not Expr_end_node:\n",
    "        child_values = [forward(child) for child in node.childs]\n",
    "        if len(node.childs) == 1:\n",
    "            # product of inner derivatives\n",
    "            # value =parant_node derivative(all of parant's childs feeded forward)\n",
    "            # new_value = derivative_of_current_node(all of current cilds feeded forward)\n",
    "            new_value = node.backward_func(*child_values)\n",
    "            if value.ndim == 0 or new_value.ndim == 0:\n",
    "                backward(node.childs[0], value * new_value)\n",
    "            else: \n",
    "                backward(node.childs[0], value @ new_value) # @ is matrix product\n",
    "            \n",
    "        else:\n",
    "            for child, new_value in zip(node.childs, node.backward_func(*child_values), strict=True):\n",
    "                if value.ndim == 0 or new_value.ndim == 0:\n",
    "                    backward(child, value * new_value)\n",
    "                else: \n",
    "                    backward(child, value @ new_value ) # @ is matrix product                  \n",
    "    else: \n",
    "        node.grad_value += value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2441ffd-91ff-4641-af26-1dff80fb85b6",
   "metadata": {},
   "source": [
    "# Example functions\n",
    "\n",
    "The following cells are structured as follows: \n",
    "- Each cell contains one example function. \\\n",
    "For each function, we define the operators, parameters, and the function itself. The operators are defined as their respective operator classes. \\\n",
    "Example: \n",
    "| mathematical operator | operator class  |\n",
    "| --- | --- |\n",
    "| + | Add() |\n",
    "| $\\cdot$ | Multiply()|\n",
    "| sin() | Sin() | \n",
    "\n",
    "- This function will then be depicted as a node tree via the $\\texttt{print\\_graph}$ function.\n",
    "- Finally, we perform the forward and backward propagation. The values of the propagations will be compared with values of the analytically solven function and derivative(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e505c03c-514b-4ad7-8a06-1ddb44b6a65b",
   "metadata": {},
   "source": [
    "## Parameters and mathematical operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de80b1fe-e59f-4aac-85da-40ef541cc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Expr_end_node(np.random.rand(1))\n",
    "x2 = Expr_end_node(np.random.rand(1))\n",
    "x = Expr_end_node(np.random.rand(1))\n",
    "w = Expr_end_node(np.random.rand(3,3))\n",
    "xv = Expr_end_node(np.random.rand(3))\n",
    "b = Expr_end_node(np.random.rand(3))\n",
    "\n",
    "add = Add()\n",
    "add_2 = Add_scalar(2)\n",
    "multiply = Multiply()\n",
    "multiply_3 = Multiply_scalar(3)\n",
    "multiply_4 = Multiply_scalar(4)\n",
    "sin = Sin()\n",
    "log = Log()\n",
    "tanh = Tanh()\n",
    "#vecadd = Vector_vector_sum() \n",
    "#matmul = Matrix_vector_product()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc2b3b6-96d4-4bf5-a765-fe76cfe7a219",
   "metadata": {},
   "source": [
    "## Function 1: $f(x_1,x_2) = \\log(x_1 \\cdot x_2) \\cdot \\sin(x_2) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99f579f8-ac29-4da5-9fe7-e63634a8244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| forward(func1): array([-0.4222195])\n",
      "ic| np.log(x1.value * x2.value) * np.sin(x2.value): array([-0.4222195])\n",
      "ic| x1.grad_value: array([0.33950571])\n",
      "ic| np.sin(x2.value) / x1.value: array([0.33950571])\n",
      "ic| x2.grad_value: array([-0.68605395])\n",
      "ic| np.sin(x2.value) / x2.value + np.log(x1.value * x2.value) * np.cos(x2.value): array([-0.68605395])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 1\n",
      "Calculus values of x1 derivative and x2 derivative: [0.33950571] [-0.68605395]\n",
      "Compared to derivatives through chain rule:         [0.33950571] [-0.68605395]\n"
     ]
    }
   ],
   "source": [
    "# defining the function\n",
    "func1 = multiply(log(multiply(x1, x2)), sin(x2))\n",
    "\n",
    "# graphical depiction\n",
    "graph1 = graphviz.Digraph('graph1', comment='test') \n",
    "graph1.attr(rankdir=\"LR\")\n",
    "print_graph(func1, graph1)\n",
    "graph1.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "\n",
    "# analytical function and its derivative(s)\n",
    "mfunc1 = np.log(x1.value * x2.value) * np.sin(x2.value) # analytical function\n",
    "mdfunc1dx1 = np.sin(x2.value) / x1.value # analytical derivative w.r.t. x1 \n",
    "mdfunc1dx2 = np.sin(x2.value) / x2.value + np.log(x1.value * x2.value) * np.cos(x2.value) # analytical derivative w.r.t. x2\n",
    "\n",
    "# comparison to analytical value\n",
    "# comparison 1: forward propagation\n",
    "ic(forward(func1)) # value of the function via forward propagation\n",
    "ic(np.log(x1.value * x2.value) * np.sin(x2.value)) # value of the analytical function\n",
    "\n",
    "# comparison 2: backward propagation\n",
    "# as we reuse the same parameter names for multiple functions, we set the derivatives w.r.t. the parameters to zero, before performing the derivatives.\n",
    "x1.grad_value=0\n",
    "x2.grad_value=0\n",
    "backward(func1) # performing the derivative via backward propagation\n",
    "ic(x1.grad_value) # value of the derivative w.r.t. x1 via backward propagation\n",
    "ic(np.sin(x2.value) / x1.value) # value of the analytical derivative w.r.t. x1 \n",
    "ic(x2.grad_value) # value of the derivative w.r.t. x2 via backward propagation\n",
    "ic(np.sin(x2.value) / x2.value + np.log(x1.value * x2.value) * np.cos(x2.value)) # value of the analytical derivative w.r.t. x2\n",
    "\n",
    "\n",
    "# Result\n",
    "print(\"function 1\")\n",
    "print(\"Calculus values of x1 derivative and x2 derivative:\", mdfunc1dx1, mdfunc1dx2 )\n",
    "print(\"Compared to derivatives through chain rule:        \", x1.grad_value, x2.grad_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f7dd8-e86c-49d9-b29f-1c4f34845236",
   "metadata": {},
   "source": [
    "## Function 2: $g(x_1, x_2) = x_1 \\cdot x_2 (x_1 + x_2) $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbe59784-a76f-4e17-a37a-76d8ae6655a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| forward(func2): array([0.17161258])\n",
      "ic| x1.value * x2.value * (x1.value + x2.value): array([0.17161258])\n",
      "ic| x1.grad_value: array([0.41607981])\n",
      "ic| mdfunc2dx1: array([0.41607981])\n",
      "ic| x2.grad_value: array([0.87295034])\n",
      "ic| mdfunc2dx2: array([0.87295034])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 2\n",
      "Calculus values of x1 derivative and x2 derivative: [0.41607981] [0.87295034]\n",
      "Compared to derivatives through chain rule:         [0.41607981] [0.87295034]\n"
     ]
    }
   ],
   "source": [
    "# defining the function\n",
    "func2 = multiply(x1, multiply(x2, add(x1, x2)))\n",
    "\n",
    "# graphical depiction\n",
    "graph2 = graphviz.Digraph('graph2', comment='test') \n",
    "graph2.attr(rankdir=\"LR\")\n",
    "print_graph(func2, graph2)\n",
    "graph2.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "# analytical function and its derivative(s)\n",
    "mfunc2 = x1.value * x2.value * (x1.value + x2.value)\n",
    "mdfunc2dx1 = x2.value * (x1.value + x2.value) + x1.value * x2.value\n",
    "mdfunc2dx2 = x1.value * (x1.value + x2.value) + x1.value * x2.value\n",
    "\n",
    "# comparison\n",
    "ic(forward(func2))\n",
    "ic(x1.value * x2.value * (x1.value + x2.value))\n",
    "\n",
    "x1.grad_value=0\n",
    "x2.grad_value=0\n",
    "backward(func2)\n",
    "ic(x1.grad_value)\n",
    "ic(mdfunc2dx1)\n",
    "ic(x2.grad_value)\n",
    "ic(mdfunc2dx2)\n",
    "\n",
    "# result\n",
    "print(\"Function 2\")\n",
    "print(\"Calculus values of x1 derivative and x2 derivative:\", mdfunc2dx1, mdfunc2dx2 )\n",
    "print(\"Compared to derivatives through chain rule:        \", x1.grad_value, x2.grad_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de858bf-d98b-4bd3-ad5d-1e63e2afc067",
   "metadata": {},
   "source": [
    "## Function 3: $h(x) = 3x^2 + 4x + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccb60fa2-89da-453d-9d58-65b3371e3041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| forward(func3): array([2.67543434])\n",
      "ic| 3*x.value**2 + 4 * x.value + 2: array([2.67543434])\n",
      "ic| mdfunc3dx: array([4.9097059])\n",
      "ic| x.grad_value: array([4.9097059])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 3\n",
      "Calculus values of x derivative:                    [4.9097059]\n",
      "Compared to derivatives through chain rule:         [4.9097059]\n"
     ]
    }
   ],
   "source": [
    "# defining the function\n",
    "func3 = add_2( add( multiply_3(multiply(x,x)) , multiply_4(x) ))\n",
    "\n",
    "# graphical depiction\n",
    "graph3 = graphviz.Digraph('graph3', comment='test') \n",
    "graph3.attr(rankdir=\"LR\")\n",
    "print_graph(func3, graph3)\n",
    "graph3.render(directory='graph_out/tt', view=False)\n",
    "\n",
    "# comparison\n",
    "mfunc3 = 3*x.value**2 + 4 * x.value + 2\n",
    "mdfunc3dx = 6 * x.value + 4\n",
    "\n",
    "ic(forward(func3))\n",
    "ic(3*x.value**2 + 4 * x.value + 2)\n",
    "\n",
    "x.grad_value = 0\n",
    "backward(func3)\n",
    "ic(mdfunc3dx)\n",
    "ic(x.grad_value)\n",
    "\n",
    "\n",
    "# Result\n",
    "print(\"Function 3\")\n",
    "print(\"Calculus values of x derivative:                   \", mdfunc3dx)\n",
    "print(\"Compared to derivatives through chain rule:        \", x.grad_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70448188-cd85-4e37-8388-c663505fd4ff",
   "metadata": {},
   "source": [
    "## Function 4: Neuron$(\\vec x, w, \\vec b) = \\tanh(w\\cdot \\vec x + \\vec b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dd5f63-2b52-4171-a6a8-c7834787e4ec",
   "metadata": {},
   "source": [
    "### Manual algorithm\n",
    "For the neuron activation function, we will compare the derivatives with what $\\texttt{torch}$ computes. \\\n",
    "The backpropagation will include the loss function as the outermost derivative, so that we have\n",
    "$$\n",
    " \\frac{\\partial \\text{Loss}(\\vec y)}{\\partial \\vartheta_{ij}} = \\frac{\\partial \\text{Loss}}{\\partial \\vec y}  \\frac{\\partial \\vec y}{\\partial \\vartheta_{ij}}\n",
    "$$\n",
    "with $\\vartheta_{ij}$ either being the weight matrix $w_{ij}$ or the bias vector $\\vec b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f98c7003-6e8b-4030-8f9d-bc1686db4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function   \n",
    "def loss_function(y_pred, y_target):\n",
    "     return (y_pred - y_target).pow(2).sum()\n",
    "    \n",
    "# (d loss)/(d y_pred) = 2*(y_pred - y_target)\n",
    "def loss_backwards(y_pred, y_target):\n",
    "     return 2*(y_pred - y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a343dfe4-148e-468b-b0b4-62cd740b98e5",
   "metadata": {},
   "source": [
    "### One-dimensional variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "951d957b-ce21-411d-ac75-54f845eb2636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| grad_ws: array([0.13392936])\n",
      "ic| grad_xs: array([0.27181791])\n",
      "ic| grad_bs: array([0.2752326])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 4 (scalar)\n",
      "Derivatives through chain rule:        \n",
      " ws :\n",
      " [0.13392936] \n",
      " xs: \n",
      " [0.27181791] \n",
      " bs: \n",
      " [0.2752326]\n"
     ]
    }
   ],
   "source": [
    "xs_value = np.random.rand(1)\n",
    "ws_value = np.random.rand(1)\n",
    "bs_value = np.random.rand(1)\n",
    "func4s_target_value = np.random.rand(1)\n",
    "\n",
    "ws = Expr_end_node(ws_value)\n",
    "xs = Expr_end_node(xs_value)\n",
    "bs = Expr_end_node(bs_value)\n",
    "func4s_target = Expr_end_node(func4s_target_value)\n",
    "\n",
    "tanh = Tanh()\n",
    "add = Add()\n",
    "multiply = Multiply()\n",
    "\n",
    "# defining the function\n",
    "func4s = tanh(add(multiply(ws,xs) , bs)) # = function 4\n",
    "\n",
    "# prediction value for loss function\n",
    "func4s_predict = forward(func4s)\n",
    "\n",
    "# graphical depiction\n",
    "graph4s = graphviz.Digraph('graph4s', comment='test') \n",
    "graph4s.attr(rankdir=\"LR\")\n",
    "print_graph(func4s, graph4s)\n",
    "graph4s.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "\n",
    "ws.grad_value = np.float64(0)\n",
    "xs.grad_value = np.float64(0)\n",
    "bs.grad_value = np.float64(0)\n",
    "backward(func4s)\n",
    "\n",
    "grad_ws = loss_backwards(func4s_predict, func4s_target.value)*ws.grad_value\n",
    "grad_xs = loss_backwards(func4s_predict, func4s_target.value)*xs.grad_value\n",
    "grad_bs = loss_backwards(func4s_predict, func4s_target.value)*bs.grad_value\n",
    "\n",
    "ic(grad_ws)\n",
    "ic(grad_xs)\n",
    "ic(grad_bs)\n",
    "\n",
    "\n",
    "# Result\n",
    "print(\"Function 4 (scalar)\")\n",
    "print(\"Derivatives through chain rule:        \\n\", \"ws :\\n\",\n",
    "      grad_ws, \"\\n xs: \\n\",\n",
    "      grad_xs, \"\\n bs: \\n\", \n",
    "      grad_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ac0c8-34f7-4172-a980-f97cfb380f1f",
   "metadata": {},
   "source": [
    "### Multidimensional values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3d47224-5f39-4954-9200-2c5aaa427f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| grad_w_cs: array([[-0.11345782, -0.12900994],\n",
      "                      [ 0.05637733,  0.06410519],\n",
      "                      [ 0.19051295,  0.21662731]])\n",
      "ic| grad_xv_cs: array([ 0.20828598, -0.02672952])\n",
      "ic| grad_b_cs: array([-0.218523  ,  0.10858434,  0.36693335])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 4\n",
      "Derivatives through chain rule:        \n",
      " w :\n",
      " [[-0.11345782 -0.12900994]\n",
      " [ 0.05637733  0.06410519]\n",
      " [ 0.19051295  0.21662731]] \n",
      " xv: \n",
      " [ 0.20828598 -0.02672952] \n",
      " b: \n",
      " [-0.218523    0.10858434  0.36693335]\n"
     ]
    }
   ],
   "source": [
    "# multidimensional values\n",
    "w_value = np.random.rand(3,2)\n",
    "xv_value = np.random.rand(2)\n",
    "b_value = np.random.rand(3)\n",
    "func4_target_value = np.random.rand(3)\n",
    "\n",
    "w = Expr_end_node(w_value)\n",
    "xv = Expr_end_node(xv_value)\n",
    "b = Expr_end_node(b_value)\n",
    "func4_target = Expr_end_node(func4_target_value)\n",
    "\n",
    "tanh = Tanh()\n",
    "vecadd = Vector_vector_sum()\n",
    "matmul = Matrix_vector_product()\n",
    "\n",
    "# defining the function\n",
    "activation = Matrix_w_x_b()\n",
    "func4 = tanh(activation(w,xv,b)) # = function 4\n",
    "\n",
    "# prediction value for loss function\n",
    "func4_predict = forward(func4)\n",
    "\n",
    "# graphical depiction\n",
    "graph4 = graphviz.Digraph('graph4', comment='test') \n",
    "graph4.attr(rankdir=\"LR\")\n",
    "print_graph(func4, graph4)\n",
    "graph4.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "\n",
    "w.grad_value = np.float64(0)\n",
    "xv.grad_value = np.float64(0)\n",
    "b.grad_value = np.float64(0)\n",
    "backward(func4)\n",
    "\n",
    "grad_w = loss_backwards(func4_predict, func4_target.value)@w.grad_value\n",
    "grad_xv = loss_backwards(func4_predict, func4_target.value)@xv.grad_value\n",
    "grad_b = loss_backwards(func4_predict, func4_target.value)@b.grad_value\n",
    "\n",
    "# reshaping the derivatives to the resepective parameter\n",
    "grad_w_cs = grad_w.reshape(w.value.shape)\n",
    "grad_xv_cs = grad_xv.reshape(xv.value.shape)\n",
    "grad_b_cs = grad_b.reshape(b.value.shape)\n",
    "\n",
    "ic(grad_w_cs)\n",
    "ic(grad_xv_cs)\n",
    "ic(grad_b_cs)\n",
    "\n",
    "\n",
    "# Result\n",
    "print(\"Function 4\")\n",
    "print(\"Derivatives through chain rule:        \\n\", \"w :\\n\",\n",
    "      grad_w_cs, \"\\n xv: \\n\",\n",
    "      grad_xv_cs, \"\\n b: \\n\", \n",
    "      grad_b_cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d13cb6-0531-4db1-83fa-0f48f5f2726d",
   "metadata": {},
   "source": [
    "### Function 4 extension: multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ffd5a03-8a83-4150-bb26-8b65146d70f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| grad_w2_rs: array([[0.07755232, 0.12375362, 0.06961662],\n",
      "                       [0.09566422, 0.15265556, 0.08587518],\n",
      "                       [0.10144618, 0.1618821 , 0.0910655 ]])\n",
      "ic| grad_b2_rs: array([0.13295109, 0.16400104, 0.1739133 ])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.13295109, 0.16400104, 0.1739133 ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2_value = np.random.rand(3,3)\n",
    "b2_value = np.random.rand(3)\n",
    "func4_2_target_value = np.random.rand(3)\n",
    "\n",
    "w2 = Expr_end_node(w2_value)\n",
    "b2 = Expr_end_node(b2_value)\n",
    "func4_2_target = Expr_end_node(func4_2_target_value)\n",
    "\n",
    "# second neuron\n",
    "func4_2 = tanh(activation(w2, func4, b2))\n",
    "\n",
    "# graphical depiction\n",
    "graph4_2 = graphviz.Digraph('graph4.2', comment='test') \n",
    "graph4_2.attr(rankdir=\"LR\")\n",
    "print_graph(func4_2, graph4_2)\n",
    "graph4_2.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "w2.grad_value = np.float64(0)\n",
    "b2.grad_value = np.float64(0)\n",
    "w.grad_value = np.float64(0)\n",
    "b.grad_value = np.float64(0)\n",
    "backward(func4_2)\n",
    "\n",
    "grad_w2 = loss_backwards(func4_2_predict, func4_2_target.value)@w2.grad_value\n",
    "grad_b2 = loss_backwards(func4_2_predict, func4_2_target.value)@b2.grad_value\n",
    "\n",
    "# reshaping the derivatives to the resepective parameter\n",
    "grad_w2_rs = grad_w2.reshape(w2.value.shape)\n",
    "grad_b2_rs = grad_b2.reshape(b2.value.shape)\n",
    "\n",
    "ic(grad_w2_rs)\n",
    "ic(grad_b2_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce921d3-14bf-4517-b4aa-5ec8b85dfa76",
   "metadata": {},
   "source": [
    "### Comparison to $\\texttt{torch}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b208f02-9a06-4dd2-8d76-6cea60fa342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91bc428-79a2-4524-aca7-9d084501a2e0",
   "metadata": {},
   "source": [
    "### One-dimensional and single-layer comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3fe39420-de53-48c5-beae-519555cc4d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| grad_ws: array([0.04336008])\n",
      "ic| grad_bs: array([0.14948149])\n",
      "ic| ws_t.grad: tensor([0.0434], dtype=torch.float64)\n",
      "ic| bs_t.grad: tensor([0.1495], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 4 (scalar)\n",
      "torch values for w and b: \n",
      " w: \n",
      " tensor([0.0434], dtype=torch.float64) \n",
      " b: \n",
      " tensor([0.1495], dtype=torch.float64) \n",
      "\n",
      "compared to manual derivatives through chain rule: \n",
      " w :\n",
      " [0.04336008] \n",
      " b: \n",
      " [0.14948149]\n"
     ]
    }
   ],
   "source": [
    "# Same code as above, but the random parameters need to be in the same cell for comparison\n",
    "# Everything inside the lined box is a repetition, skip to torch code\n",
    "# -------------------------------------------------------------------------------------------\n",
    "# actual values\n",
    "xs_value = np.random.rand(1)\n",
    "ws_value = np.random.rand(1)\n",
    "bs_value = np.random.rand(1)\n",
    "func4s_target_value = np.random.rand(1)\n",
    "\n",
    "ws.grad_value = np.float64(0)\n",
    "xs.grad_value = np.float64(0)\n",
    "bs.grad_value = np.float64(0)\n",
    "\n",
    "ws = Expr_end_node(ws_value)\n",
    "xs = Expr_end_node(xs_value)\n",
    "bs = Expr_end_node(bs_value)\n",
    "func4s_target = Expr_end_node(func4s_target_value)\n",
    "\n",
    "tanh = Tanh()\n",
    "add = Add()\n",
    "multiply = Multiply()\n",
    "\n",
    "# defining the function\n",
    "func4s = tanh(add(multiply(ws,xs) , bs)) # = function 4\n",
    "\n",
    "# prediction value for loss function\n",
    "func4s_predict = forward(func4s)\n",
    "\n",
    "ws.grad_value = np.float64(0)\n",
    "xs.grad_value = np.float64(0)\n",
    "bs.grad_value = np.float64(0)\n",
    "backward(func4s)\n",
    "\n",
    "grad_ws = loss_backwards(func4s_predict, func4s_target.value)*ws.grad_value\n",
    "grad_xs = loss_backwards(func4s_predict, func4s_target.value)*xs.grad_value\n",
    "grad_bs = loss_backwards(func4s_predict, func4s_target.value)*bs.grad_value\n",
    "\n",
    "# -------------------------------------------------------------------------------------------\n",
    "# here starts the torch code\n",
    "def forward_function(x, w, b):\n",
    "     return torch.tanh(w * x + b)\n",
    "\n",
    "# torch expression for parameters\n",
    "xs_t = torch.tensor(xs_value, requires_grad=True)\n",
    "ws_t = torch.tensor(ws_value, requires_grad=True)\n",
    "bs_t = torch.tensor(bs_value, requires_grad=True)\n",
    "\n",
    "\n",
    "func4s_target_t = torch.tensor(func4s_target_value)\n",
    "\n",
    "func4s_predict_t = forward_function(xs_t, ws_t, bs_t,)\n",
    "\n",
    "loss_t = loss_function(func4s_predict_t, func4s_target_t)\n",
    "\n",
    "loss_t.backward()\n",
    "\n",
    "ic(grad_ws)\n",
    "ic(grad_bs)\n",
    "ic(ws_t.grad)\n",
    "ic(bs_t.grad)\n",
    "\n",
    "\n",
    "# Result\n",
    "print(\"Function 4 (scalar)\")\n",
    "print(\"torch values for w and b: \\n\",\n",
    "      \"w: \\n\", ws_t.grad, \"\\n\",\n",
    "      \"b: \\n\", bs_t.grad, \"\\n\")\n",
    "print(\"compared to manual derivatives through chain rule: \\n\", \n",
    "      \"w :\\n\", grad_ws, \"\\n\",\n",
    "      \"b: \\n\", grad_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952366a-5a0e-4e31-a348-475369be10fd",
   "metadata": {},
   "source": [
    "### Multidimensional and multilayer comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f3af9b4-44f8-45cb-a7fe-836cc07ca1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| grad_w2_rs: array([[0.02405814, 0.04238482, 0.04738375],\n",
      "                       [0.01858132, 0.03273595, 0.03659687],\n",
      "                       [0.03645592, 0.06422682, 0.07180181]])\n",
      "ic| grad_b2_rs: array([0.05003356, 0.03864346, 0.07581715])\n",
      "ic| w2_t.grad: tensor([[0.0241, 0.0424, 0.0474],\n",
      "                       [0.0186, 0.0327, 0.0366],\n",
      "                       [0.0365, 0.0642, 0.0718]], dtype=torch.float64)\n",
      "ic| b2_t.grad: tensor([0.0500, 0.0386, 0.0758], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 4\n",
      "\n",
      " Input layer: \n",
      " w: \n",
      " [[0.76986411 0.18741674]\n",
      " [0.86170516 0.12539191]\n",
      " [0.3849205  0.9887739 ]] \n",
      " b: \n",
      " [0.16677814 0.92080612 0.82980949]\n",
      "\n",
      " Second layer:\n",
      "torch values for w and b: \n",
      " w: \n",
      " tensor([[0.0211, 0.0752],\n",
      "        [0.0062, 0.0223],\n",
      "        [0.0020, 0.0073]], dtype=torch.float64) \n",
      " b: \n",
      " tensor([0.0848, 0.0251, 0.0083], dtype=torch.float64) \n",
      "\n",
      "compared to manual derivatives through chain rule: \n",
      " w :\n",
      " [[0.02105467 0.07523013]\n",
      " [0.00623562 0.02228041]\n",
      " [0.00204865 0.00731998]] \n",
      " b: \n",
      " [0.08482723 0.02512272 0.00825379]\n",
      "\n",
      " Third layer:\n",
      "torch values for w and b: \n",
      " w: \n",
      " tensor([[0.0211, 0.0752],\n",
      "        [0.0062, 0.0223],\n",
      "        [0.0020, 0.0073]], dtype=torch.float64) \n",
      " b: \n",
      " tensor([0.0848, 0.0251, 0.0083], dtype=torch.float64) \n",
      "\n",
      "compared to manual derivatives through chain rule: \n",
      " w :\n",
      " [[0.02105467 0.07523013]\n",
      " [0.00623562 0.02228041]\n",
      " [0.00204865 0.00731998]] \n",
      " b: \n",
      " [0.08482723 0.02512272 0.00825379]\n"
     ]
    }
   ],
   "source": [
    "# Same code as above, but the random parameters need to be in the same cell for comparison\n",
    "# Everything inside the lined box is a repetition, skip to torch code\n",
    "# -------------------------------------------------------------------------------------------\n",
    "# actual values\n",
    "w_value = np.random.rand(3,2)\n",
    "xv_value = np.random.rand(2)\n",
    "b_value = np.random.rand(3)\n",
    "\n",
    "w2_value = np.random.rand(3,3)\n",
    "b2_value = np.random.rand(3)\n",
    "func4_2_target_value = np.random.rand(3)\n",
    "\n",
    "# node expression\n",
    "w = Expr_end_node(w_value)\n",
    "xv = Expr_end_node(xv_value)\n",
    "b = Expr_end_node(b_value)\n",
    "\n",
    "w2 = Expr_end_node(w2_value)\n",
    "b2 = Expr_end_node(b2_value)\n",
    "\n",
    "\n",
    "func4_2_target = Expr_end_node(func4_2_target_value)\n",
    "\n",
    "\n",
    "tanh = Tanh()\n",
    "vecadd = Vector_vector_sum()\n",
    "matmul = Matrix_vector_product()\n",
    "activation = Matrix_w_x_b()\n",
    "\n",
    "func4 = tanh(activation(w,xv,b))\n",
    "func4_2 = tanh(activation(w2, func4, b2))\n",
    "func4_2_predict = forward(func4_2)\n",
    "\n",
    "w.grad_value = np.float64(0)\n",
    "xv.grad_value = np.float64(0)\n",
    "b.grad_value = np.float64(0)\n",
    "w2.grad_value = np.float64(0)\n",
    "b2.grad_value = np.float64(0)\n",
    "backward(func4_2)\n",
    "\n",
    "grad_w = loss_backwards(func4_2_predict, func4_2_target.value)@w.grad_value\n",
    "grad_b = loss_backwards(func4_2_predict, func4_2_target.value)@b.grad_value\n",
    "\n",
    "grad_w2 = loss_backwards(func4_2_predict, func4_2_target.value)@w2.grad_value\n",
    "grad_b2 = loss_backwards(func4_2_predict, func4_2_target.value)@b2.grad_value\n",
    "\n",
    "# reshaping the derivatives to the resepective parameter\n",
    "grad_w_rs = grad_w.reshape(w.value.shape)\n",
    "grad_b_rs = grad_b.reshape(b.value.shape)\n",
    "grad_w2_rs = grad_w2.reshape(w2.value.shape)\n",
    "grad_b2_rs = grad_b2.reshape(b2.value.shape)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------\n",
    "# here starts the torch code\n",
    "def forward_function(x, w1, b1, w2, b2):\n",
    "     return torch.tanh(w2 @ torch.tanh(w1 @ x + b1) + b2)\n",
    "\n",
    "# torch expression for parameters\n",
    "xv_t = torch.tensor(xv_value, requires_grad=True)\n",
    "w_t = torch.tensor(w_value, requires_grad=True)\n",
    "b_t = torch.tensor(b_value, requires_grad=True)\n",
    "\n",
    "w2_t = torch.tensor(w2_value, requires_grad=True)\n",
    "b2_t = torch.tensor(b2_value, requires_grad=True)\n",
    "\n",
    "\n",
    "func4_2_target_t = torch.tensor(func4_2_target_value)\n",
    "\n",
    "func4_2_predict_t = forward_function(xv_t, w_t, b_t, w2_t, b2_t)\n",
    "loss_t = loss_function(func4_2_predict_t, func4_2_target_t)\n",
    "\n",
    "loss_t.backward()\n",
    "\n",
    "ic(grad_w2_rs)\n",
    "ic(grad_b2_rs)\n",
    "ic(w2_t.grad)\n",
    "ic(b2_t.grad)\n",
    "\n",
    "print(\"Function 4\")\n",
    "print(\"\\n Input layer: \\n\",\n",
    "      \"w: \\n\", w_value, \"\\n\",\n",
    "      \"b: \\n\", b_value)\n",
    "print(\"\\n Second layer:\")\n",
    "print(\"torch values for w and b: \\n\",\n",
    "      \"w: \\n\", w_t.grad, \"\\n\",\n",
    "      \"b: \\n\", b_t.grad, \"\\n\")\n",
    "print(\"compared to manual derivatives through chain rule: \\n\", \n",
    "      \"w :\\n\", grad_w_rs, \"\\n\",\n",
    "      \"b: \\n\", grad_b_rs)\n",
    "print(\"\\n Third layer:\")\n",
    "print(\"torch values for w and b: \\n\",\n",
    "      \"w: \\n\", w_t.grad, \"\\n\",\n",
    "      \"b: \\n\", b_t.grad, \"\\n\")\n",
    "print(\"compared to manual derivatives through chain rule: \\n\", \n",
    "      \"w :\\n\", grad_w_rs, \"\\n\",\n",
    "      \"b: \\n\", grad_b_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3dfa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
