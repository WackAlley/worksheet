{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d7a675-fc31-4a1a-a56f-f7128802c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f6ed9-4711-4b53-8ec4-b5b6bf682edd",
   "metadata": {},
   "source": [
    "# Modules\n",
    "- To better organize our project, we outsourced certain components that support the main code presented in this notebook.\n",
    "\n",
    "### Atomic Operations, and Expression Rree\n",
    "\n",
    "- Each atomic operation is implemented as a separate class.  \n",
    "- Mathematical functions can be built by chaining these atomic operations together.  \n",
    "- These functions are internally represented using an *expression tree*.  \n",
    "  - In the expression tree:  \n",
    "    - Each atomic operation is a node, more specifically an instance of $\\texttt{Expr\\_node()}$.  \n",
    "    - Each variable in the function is also represented as a node, more specifically an instance of $\\texttt{Expr\\_end\\_node()}$. \n",
    " - the classes of atomic operation all inherit from the same abstract classes, which defines their sturcture:\n",
    "     - all operations have a `forward` and a `backward` function, where the backward function is the derivative of the forward function \n",
    "      - These classes are callable. Calling an operation object on another value automatically builds the *expression tree*.  \n",
    "  - To calculate the `forward pass`, the expression tree is traversed using the `forward` function.  \n",
    "  - The `backward pass` requires both the `forward` and `backward` functions to compute derivatives, applying the **chain rule** during traversal.\n",
    "  \n",
    "### Visualize the Expression Tree\n",
    "- $\\texttt{print\\_graph}$: module provides everything needed to visualize the expression tree using the **Graphviz** package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0e12b73-3264-48c8-abe4-a9d3dab1702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forward_backward_functions_and_nodes import * \n",
    "from print_graph import print_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af883d-fb84-4c7d-9503-d86bfd890234",
   "metadata": {},
   "source": [
    "#  Details on our Forward and Backward Propagation Algorithms\n",
    "Both the forward and the backward pass are recursive functions.\n",
    "The forward pass computes the value of the specified node, this is the evaluation of the function at the points (values) of the end nodes. If the node is not an endnode, it will keep recurring itself until it reaches one. \\\n",
    "The backward propagation calculates the outer derivative at the current node by evaluating the backward function at the values from the forward pass of all child nodes and recursively calls itself on each child node. In each recursion step, the derivative values are multiplied. The propagation continues until it reaches an end node, where the accumulated product is assigned to the value of the end node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e60e21b",
   "metadata": {},
   "source": [
    "## Backpropagtion for Vectors and Matrices\n",
    "\n",
    "For the application, especially in large neural networks, we have many parameters to manage. It is essential to group variables into matrices and vectors, allowing the backpropagation algorithm to handle them efficiently and helping the user maintain an overview.\n",
    "\n",
    "**Our algorithm is capable of handling multidimensional values, such as vectors and matrices.** According to the **multidimensional chain rule**, the derivative of a chained function is the matrix product of their Jacobian matrices. This task is **not trivial**, as it **requires defining the Jacobian matrix for matrices**.\n",
    "\n",
    "Our initial approach was to generalize the Jacobian matrix and represent it as third-order tensors, in order to preserve the 2D structure of each matrix within the Jacobian. However, this approach became confusing, as each multiplication required transposing or flattening the matrices to obtain the correct result.\n",
    "\n",
    "As a result, we decided to treat every component (or entry) of each vector and matrix that the function depends on as an independent parameter. We then used the standard definition of the Jacobian matrix, where each row contains the derivatives of the components of the destination (codomain) of the function, and for each parameter (in the domain of definition), there is one row containing the corresponding partiial derivatives. \n",
    "\n",
    "To every child node, the columns of the Jacobian corresponding to the components of the parameter that the node represents (whether vector, matrix, or scalar) are passed. The multiplication then follows the usual matrix multiplication process. Since we have one column for each component, we need to reshape the result at the end node before further use. If the codomain of the entire chained function is one-dimensional, the shape of the end node parameter can be used for reshaping. If not, there will be as many rows as there are dimensions in the codomain, and each row needs to be reshaped to the shape of the end node parameter.\n",
    "\n",
    "In the case of artificial neural networks, the mentioned entire function is the loss function chained with the network's function. Since the loss function is a scalar value for each end node parameter, the entire backpropagation process will result in a Jacobian matrix with one row and as many columns as there are entries in the parameter. This can easily be reshaped using the following pseudocode: parameter.reshape(paramer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "054932a3-ff42-4ef3-bad1-7591b1f0e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(node):\n",
    "    return node.forward_func(*(forward(child) for child in node.childs)) if type(node) is not Expr_end_node else node.value \n",
    "    \n",
    "# def backward(node, value = np.float64(1)):\n",
    "#     if type(node) is not Expr_end_node:\n",
    "#         child_values = [forward(child) for child in node.childs] # computes the argument of the outer derivative. In other words: it computes g of f'(g)\n",
    "#         if len(node.childs) == 1:\n",
    "#             new_value = node.backward_func(*child_values) # computes the outer derivative f'(g)\n",
    "#             if value.ndim == 0 or new_value.ndim == 0:\n",
    "#                 backward(node.childs[0], value * new_value)\n",
    "#             else: \n",
    "#                 backward(node.childs[0], new_value.T @ value) # @ is matrix product\n",
    "#         else:\n",
    "#             for child, new_value in zip(node.childs, node.backward_func(*child_values), strict=True):\n",
    "#                 if value.ndim == 0 or new_value.ndim == 0:\n",
    "#                     backward(child, value * new_value)\n",
    "#                 else: \n",
    "#                     backward(child, new_value.T @ value)                 \n",
    "#     else:\n",
    "#         node.grad_value += value\n",
    "\n",
    "\n",
    "def backward(node, value = np.float64(1)):\n",
    "    if type(node) is not Expr_end_node:\n",
    "        child_values = [forward(child) for child in node.childs]\n",
    "        if len(node.childs) == 1:\n",
    "            # product of inner derivatives\n",
    "            # value =parant_node derivative(all of parant's childs feeded forward)\n",
    "            # new_value = derivative_of_current_node(all of current cilds feeded forward)\n",
    "            new_value = node.backward_func(*child_values)\n",
    "            if value.ndim == 0 or new_value.ndim == 0:\n",
    "                backward(node.childs[0], value * new_value)\n",
    "            else: \n",
    "                backward(node.childs[0], value @ new_value) # @ is matrix product\n",
    "            \n",
    "        else:\n",
    "            for child, new_value in zip(node.childs, node.backward_func(*child_values), strict=True):\n",
    "                if value.ndim == 0 or new_value.ndim == 0:\n",
    "                    backward(child, value * new_value)\n",
    "                else: \n",
    "                    backward(child, value @ new_value ) # @ is matrix product                  \n",
    "    else: \n",
    "        node.grad_value += value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2441ffd-91ff-4641-af26-1dff80fb85b6",
   "metadata": {},
   "source": [
    "# Example Functions\n",
    "\n",
    "The following cells are structured as follows: \n",
    "- Each cell contains one example function. \\\n",
    "For each function, we define the operators, parameters, and the function itself. The operators are defined as their respective operator classes. \\\n",
    "Example: \n",
    "| mathematical operator | operator class  |\n",
    "| --- | --- |\n",
    "| addition (+) | Add() |\n",
    "| multiplication ($\\cdot$) | Multiply()|\n",
    "| sin() | Sin() | \n",
    "\n",
    "- This function will then be depicted as a node tree via the $\\texttt{print\\_graph}$ function.\n",
    "- Finally, we perform the forward and backward propagation. The values of the propagations will be compared with values of the analytically solved function and derivative(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e505c03c-514b-4ad7-8a06-1ddb44b6a65b",
   "metadata": {},
   "source": [
    "## Parameters and Mathematical Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de80b1fe-e59f-4aac-85da-40ef541cc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Expr_end_node(np.random.rand(1))\n",
    "x2 = Expr_end_node(np.random.rand(1))\n",
    "x = Expr_end_node(np.random.rand(1))\n",
    "w = Expr_end_node(np.random.rand(3,3))\n",
    "xv = Expr_end_node(np.random.rand(3))\n",
    "b = Expr_end_node(np.random.rand(3))\n",
    "\n",
    "add = Add()\n",
    "add_2 = Add_scalar(2)\n",
    "multiply = Multiply()\n",
    "multiply_3 = Multiply_scalar(3)\n",
    "multiply_4 = Multiply_scalar(4)\n",
    "sin = Sin()\n",
    "log = Log()\n",
    "tanh = Tanh()\n",
    "#vecadd = Vector_vector_sum() \n",
    "#matmul = Matrix_vector_product()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc2b3b6-96d4-4bf5-a765-fe76cfe7a219",
   "metadata": {},
   "source": [
    "## Function 1: $f(x_1,x_2) = \\log(x_1 \\cdot x_2) \\cdot \\sin(x_2) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99f579f8-ac29-4da5-9fe7-e63634a8244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| forward(func1): array([-0.47822975])\n",
      "ic| np.log(x1.value * x2.value) * np.sin(x2.value): array([-0.47822975])\n",
      "ic| x1.grad_value: array([0.45698546])\n",
      "ic| np.sin(x2.value) / x1.value: array([0.45698546])\n",
      "ic| x2.grad_value: array([-0.48324304])\n",
      "ic| np.sin(x2.value) / x2.value + np.log(x1.value * x2.value) * np.cos(x2.value): array([-0.48324304])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 1\n",
      "Calculus values of x1 derivative and x2 derivative:   [0.45698546] [-0.48324304]\n",
      "Compared to derivatives optained with own algorithm:  [0.45698546] [-0.48324304]\n"
     ]
    }
   ],
   "source": [
    "# defining the function\n",
    "func1 = multiply(log(multiply(x1, x2)), sin(x2))\n",
    "\n",
    "# graphical depiction\n",
    "graph1 = graphviz.Digraph('graph1', comment='test') \n",
    "graph1.attr(rankdir=\"LR\")\n",
    "print_graph(func1, graph1)\n",
    "graph1.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "\n",
    "# analytical function and its derivative(s)\n",
    "mfunc1 = np.log(x1.value * x2.value) * np.sin(x2.value) # analytical function\n",
    "mdfunc1dx1 = np.sin(x2.value) / x1.value # analytical derivative w.r.t. x1 \n",
    "mdfunc1dx2 = np.sin(x2.value) / x2.value + np.log(x1.value * x2.value) * np.cos(x2.value) # analytical derivative w.r.t. x2\n",
    "\n",
    "# comparison to analytical value\n",
    "# comparison 1: forward propagation\n",
    "ic(forward(func1)) # value of the function via forward propagation\n",
    "ic(np.log(x1.value * x2.value) * np.sin(x2.value)) # value of the analytical function\n",
    "\n",
    "# comparison 2: backward propagation\n",
    "# as we reuse the same parameter names for multiple functions, we set the derivatives w.r.t. the parameters to zero, before performing the derivatives.\n",
    "x1.grad_value=0\n",
    "x2.grad_value=0\n",
    "backward(func1) # performing the derivative via backward propagation\n",
    "ic(x1.grad_value) # value of the derivative w.r.t. x1 via backward propagation\n",
    "ic(np.sin(x2.value) / x1.value) # value of the analytical derivative w.r.t. x1 \n",
    "ic(x2.grad_value) # value of the derivative w.r.t. x2 via backward propagation\n",
    "ic(np.sin(x2.value) / x2.value + np.log(x1.value * x2.value) * np.cos(x2.value)) # value of the analytical derivative w.r.t. x2\n",
    "\n",
    "\n",
    "# Result\n",
    "print(\"function 1\")\n",
    "print(\"Calculus values of x1 derivative and x2 derivative:  \", mdfunc1dx1, mdfunc1dx2 )\n",
    "print(\"Compared to derivatives optained with own algorithm: \", x1.grad_value, x2.grad_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f7dd8-e86c-49d9-b29f-1c4f34845236",
   "metadata": {},
   "source": [
    "## Function 2: $g(x_1, x_2) = x_1 \\cdot x_2 (x_1 + x_2) $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe59784-a76f-4e17-a37a-76d8ae6655a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| forward(func2): array([0.2123992])\n",
      "ic| x1.value * x2.value * (x1.value + x2.value): array([0.2123992])\n",
      "ic| x1.grad_value: array([0.52691469])\n",
      "ic| mdfunc2dx1: array([0.52691469])\n",
      "ic| x2.grad_value: array([0.88769867])\n",
      "ic| mdfunc2dx2: array([0.88769867])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 2\n",
      "Calculus values of x1 derivative and x2 derivative:   [0.52691469] [0.88769867]\n",
      "Compared to derivatives optained with own algorithm:  [0.52691469] [0.88769867]\n"
     ]
    }
   ],
   "source": [
    "# defining the function\n",
    "func2 = multiply(x1, multiply(x2, add(x1, x2)))\n",
    "\n",
    "# graphical depiction\n",
    "graph2 = graphviz.Digraph('graph2', comment='test') \n",
    "graph2.attr(rankdir=\"LR\")\n",
    "print_graph(func2, graph2)\n",
    "graph2.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "# analytical function and its derivative(s)\n",
    "mfunc2 = x1.value * x2.value * (x1.value + x2.value)\n",
    "mdfunc2dx1 = x2.value * (x1.value + x2.value) + x1.value * x2.value\n",
    "mdfunc2dx2 = x1.value * (x1.value + x2.value) + x1.value * x2.value\n",
    "\n",
    "# comparison\n",
    "ic(forward(func2))\n",
    "ic(x1.value * x2.value * (x1.value + x2.value))\n",
    "\n",
    "x1.grad_value=0\n",
    "x2.grad_value=0\n",
    "backward(func2)\n",
    "ic(x1.grad_value)\n",
    "ic(mdfunc2dx1)\n",
    "ic(x2.grad_value)\n",
    "ic(mdfunc2dx2)\n",
    "\n",
    "# result\n",
    "print(\"Function 2\")\n",
    "print(\"Calculus values of x1 derivative and x2 derivative:  \", mdfunc2dx1, mdfunc2dx2 )\n",
    "print(\"Compared to derivatives optained with own algorithm: \", x1.grad_value, x2.grad_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de858bf-d98b-4bd3-ad5d-1e63e2afc067",
   "metadata": {},
   "source": [
    "## Function 3: $h(x) = 3x^2 + 4x + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccb60fa2-89da-453d-9d58-65b3371e3041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| forward(func3): array([2.59169226])\n",
      "ic| 3*x.value**2 + 4 * x.value + 2: array([2.59169226])\n",
      "ic| mdfunc3dx: array([4.80627789])\n",
      "ic| x.grad_value: array([4.80627789])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 3\n",
      "Calculus values of x derivative:                      [4.80627789]\n",
      "Compared to derivatives optained with own algorithm:  [4.80627789]\n"
     ]
    }
   ],
   "source": [
    "# defining the function\n",
    "func3 = add_2( add( multiply_3(multiply(x,x)) , multiply_4(x) ))\n",
    "\n",
    "# graphical depiction\n",
    "graph3 = graphviz.Digraph('graph3', comment='test') \n",
    "graph3.attr(rankdir=\"LR\")\n",
    "print_graph(func3, graph3)\n",
    "graph3.render(directory='graph_out/tt', view=False)\n",
    "\n",
    "# comparison\n",
    "mfunc3 = 3*x.value**2 + 4 * x.value + 2\n",
    "mdfunc3dx = 6 * x.value + 4\n",
    "\n",
    "ic(forward(func3))\n",
    "ic(3*x.value**2 + 4 * x.value + 2)\n",
    "\n",
    "x.grad_value = 0\n",
    "backward(func3)\n",
    "ic(mdfunc3dx)\n",
    "ic(x.grad_value)\n",
    "\n",
    "\n",
    "# Result\n",
    "print(\"Function 3\")\n",
    "print(\"Calculus values of x derivative:                     \", mdfunc3dx)\n",
    "print(\"Compared to derivatives optained with own algorithm: \", x.grad_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70448188-cd85-4e37-8388-c663505fd4ff",
   "metadata": {},
   "source": [
    "## Function 4: Neuron$(\\vec x, w, \\vec b) = \\tanh(w\\cdot \\vec x + \\vec b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a343dfe4-148e-468b-b0b4-62cd740b98e5",
   "metadata": {},
   "source": [
    "### One-dimensional Variables: Neuron$(x, w, b) = \\tanh(w\\cdot x + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "951d957b-ce21-411d-ac75-54f845eb2636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 4 (scalar)\n",
      "Derivatives optained with own algorithm:        \n",
      " ws :\n",
      " [0.327417] \n",
      " xs: \n",
      " [0.13154934] \n",
      " bs: \n",
      " 0.4124164233403246\n",
      "analytical results:        \n",
      " ws :\n",
      " [0.327417] \n",
      " xs: \n",
      " [0.13154934] \n",
      " bs: \n",
      " [0.41241642]\n"
     ]
    }
   ],
   "source": [
    "xs_value = np.random.rand(1)\n",
    "ws_value = np.random.rand(1)\n",
    "bs_value = np.random.rand(1)\n",
    "func4s_target_value = np.random.rand(1)\n",
    "\n",
    "ws = Expr_end_node(ws_value)\n",
    "xs = Expr_end_node(xs_value)\n",
    "bs = Expr_end_node(bs_value)\n",
    "func4s_target = Expr_end_node(func4s_target_value)\n",
    "\n",
    "tanh = Tanh()\n",
    "add = Add()\n",
    "multiply = Multiply()\n",
    "\n",
    "# defining the function\n",
    "func4s = tanh(add(multiply(ws,xs) , bs)) # = function 4\n",
    "\n",
    "# prediction value for loss function\n",
    "func4s_predict = forward(func4s)\n",
    "\n",
    "# graphical depiction\n",
    "graph4s = graphviz.Digraph('graph4s', comment='test') \n",
    "graph4s.attr(rankdir=\"LR\")\n",
    "print_graph(func4s, graph4s)\n",
    "graph4s.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "\n",
    "ws.grad_value = np.float64(0)\n",
    "xs.grad_value = np.float64(0)\n",
    "bs.grad_value = np.float64(0)\n",
    "backward(func4s)\n",
    "\n",
    "#grad_ws = loss_backwards(func4s_predict, func4s_target.value)*ws.grad_value\n",
    "#grad_xs = loss_backwards(func4s_predict, func4s_target.value)*xs.grad_value\n",
    "#grad_bs = loss_backwards(func4s_predict, func4s_target.value)*bs.grad_value\n",
    "#ic(grad_ws)\n",
    "#ic(grad_xs)\n",
    "#ic(grad_bs)\n",
    "\n",
    "\n",
    "# analytical function and its derivative(s)\n",
    "dfunc4dw = 1 / np.cosh(ws.value * xs.value + bs.value)**2 * xs.value\n",
    "dfunc4dx = 1 / np.cosh(ws.value * xs.value + bs.value)**2 * ws.value\n",
    "dfunc4db = 1 / np.cosh(ws.value * xs.value + bs.value)**2\n",
    "\n",
    "# Result\n",
    "print(\"Function 4 (scalar)\")\n",
    "print(\"Derivatives optained with own algorithm:        \\n\", \"ws :\\n\",\n",
    "      ws.grad_value, \"\\n xs: \\n\",\n",
    "      xs.grad_value, \"\\n bs: \\n\", \n",
    "      bs.grad_value)\n",
    "print(\"analytical results:        \\n\", \"ws :\\n\",\n",
    "      dfunc4dw, \"\\n xs: \\n\",\n",
    "      dfunc4dx, \"\\n bs: \\n\", \n",
    "      dfunc4db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dd5f63-2b52-4171-a6a8-c7834787e4ec",
   "metadata": {},
   "source": [
    "### Extending to Multiple Dimensions: $( x \\to \\vec{x} )$\n",
    "For the neuron activation function with multidimensional input, we will compare the derivatives with what $\\texttt{torch}$ computes. See further below. \\\n",
    "The backpropagation will include the loss function as the outermost function chained with the function of the neuron layer or neuronal net ($\\vec y$), so that we have\n",
    "$$\n",
    " \\frac{\\partial \\text{Loss}(\\vec y)}{\\partial \\vartheta_{ij}} = \\frac{\\partial \\text{Loss}}{\\partial \\vec y}  \\frac{\\partial \\vec y}{\\partial \\vartheta_{ij}}\n",
    "$$\n",
    "with $\\vec y = \\text{Neuron}(\\vec x, w, \\vec b)$ and $\\vartheta_{ij}$ either being the weight matrix $w_{ij}$ or the bias vector $\\vec b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f98c7003-6e8b-4030-8f9d-bc1686db4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function   \n",
    "def loss_function(y_pred, y_target):\n",
    "     return (y_pred - y_target).pow(2).sum()\n",
    "    \n",
    "# (d loss)/(d y_pred) = 2*(y_pred - y_target)\n",
    "def loss_backwards(y_pred, y_target):\n",
    "     return 2*(y_pred - y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ac0c8-34f7-4172-a980-f97cfb380f1f",
   "metadata": {},
   "source": [
    "### Multidimensional Value Neurons $(\\vec x, w, \\vec b) = \\tanh(w\\cdot \\vec x + \\vec b)$ (a whole layer of neurons in one go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3d47224-5f39-4954-9200-2c5aaa427f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| grad_w_cs: array([[ 0.19794248,  0.26151961],\n",
      "                      [ 0.24653337,  0.32571739],\n",
      "                      [-0.18034925, -0.23827561]])\n",
      "ic| grad_xv_cs: array([0.20577088, 0.36978172])\n",
      "ic| grad_b_cs: array([ 0.43476933,  0.54149643, -0.39612681])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 4\n",
      "Derivatives optained with own algorithm: :        \n",
      " w :\n",
      " [[ 0.19794248  0.26151961]\n",
      " [ 0.24653337  0.32571739]\n",
      " [-0.18034925 -0.23827561]] \n",
      " xv: \n",
      " [0.20577088 0.36978172] \n",
      " b: \n",
      " [ 0.43476933  0.54149643 -0.39612681]\n",
      "For comparison of torches results see section Comparison to torch!\n"
     ]
    }
   ],
   "source": [
    "# multidimensional values\n",
    "w_value = np.random.rand(3,2)\n",
    "xv_value = np.random.rand(2)\n",
    "b_value = np.random.rand(3)\n",
    "func4_target_value = np.random.rand(3)\n",
    "\n",
    "w = Expr_end_node(w_value)\n",
    "xv = Expr_end_node(xv_value)\n",
    "b = Expr_end_node(b_value)\n",
    "func4_target = Expr_end_node(func4_target_value)\n",
    "\n",
    "tanh = Tanh()\n",
    "vecadd = Vector_vector_sum()\n",
    "matmul = Matrix_vector_product()\n",
    "\n",
    "# defining the function\n",
    "activation = Matrix_w_x_b()\n",
    "func4 = tanh(activation(w,xv,b)) # = function 4\n",
    "\n",
    "# prediction value for loss function\n",
    "func4_predict = forward(func4)\n",
    "\n",
    "# graphical depiction\n",
    "graph4 = graphviz.Digraph('graph4', comment='test') \n",
    "graph4.attr(rankdir=\"LR\")\n",
    "print_graph(func4, graph4)\n",
    "graph4.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "\n",
    "w.grad_value = np.float64(0)\n",
    "xv.grad_value = np.float64(0)\n",
    "b.grad_value = np.float64(0)\n",
    "backward(func4)\n",
    "\n",
    "grad_w = loss_backwards(func4_predict, func4_target.value)@w.grad_value\n",
    "grad_xv = loss_backwards(func4_predict, func4_target.value)@xv.grad_value\n",
    "grad_b = loss_backwards(func4_predict, func4_target.value)@b.grad_value\n",
    "\n",
    "# reshaping the derivatives to the resepective parameter\n",
    "grad_w_cs = grad_w.reshape(w.value.shape)\n",
    "grad_xv_cs = grad_xv.reshape(xv.value.shape)\n",
    "grad_b_cs = grad_b.reshape(b.value.shape)\n",
    "\n",
    "ic(grad_w_cs)\n",
    "ic(grad_xv_cs)\n",
    "ic(grad_b_cs)\n",
    "\n",
    "\n",
    "# Result\n",
    "print(\"Function 4\")\n",
    "print(\"Derivatives optained with own algorithm: :        \\n\", \"w :\\n\",\n",
    "      grad_w_cs, \"\\n xv: \\n\",\n",
    "      grad_xv_cs, \"\\n b: \\n\", \n",
    "      grad_b_cs)\n",
    "print(\"For comparison of torches results see section Comparison to torch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d13cb6-0531-4db1-83fa-0f48f5f2726d",
   "metadata": {},
   "source": [
    "### Function 4 Extension: Multiple Layers\n",
    "\n",
    "In proof to show the capabilty of our algorithm to handle deep neuronal natwork functions.\n",
    "We have here a simple implementation of a **fully connected feed forward network with 3 layers (input, hidden and output)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ffd5a03-8a83-4150-bb26-8b65146d70f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| grad_w2_rs: array([[0.13627391, 0.09196742, 0.10082281],\n",
      "                       [0.34560476, 0.23323891, 0.25569708],\n",
      "                       [0.17022174, 0.11487785, 0.12593924]])\n",
      "ic| grad_b2_rs: array([0.16504418, 0.41856913, 0.2061591 ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For comparison of torches results see section Comparison to torch!\n"
     ]
    }
   ],
   "source": [
    "w2_value = np.random.rand(3,3)\n",
    "b2_value = np.random.rand(3)\n",
    "func4_2_target_value = np.random.rand(3)\n",
    "\n",
    "w2 = Expr_end_node(w2_value)\n",
    "b2 = Expr_end_node(b2_value)\n",
    "func4_2_target = Expr_end_node(func4_2_target_value)\n",
    "\n",
    "# second neuron\n",
    "func4_2 = tanh(activation(w2, func4, b2))\n",
    "\n",
    "# graphical depiction\n",
    "graph4_2 = graphviz.Digraph('graph4.2', comment='test') \n",
    "graph4_2.attr(rankdir=\"LR\")\n",
    "print_graph(func4_2, graph4_2)\n",
    "graph4_2.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "w2.grad_value = np.float64(0)\n",
    "b2.grad_value = np.float64(0)\n",
    "w.grad_value = np.float64(0)\n",
    "b.grad_value = np.float64(0)\n",
    "backward(func4_2)\n",
    "\n",
    "func4_2 = tanh(activation(w2, func4, b2))\n",
    "func4_2_predict = forward(func4_2)\n",
    "\n",
    "grad_w2 = loss_backwards(func4_2_predict, func4_2_target.value)@w2.grad_value\n",
    "grad_b2 = loss_backwards(func4_2_predict, func4_2_target.value)@b2.grad_value\n",
    "\n",
    "# reshaping the derivatives to the resepective parameter\n",
    "grad_w2_rs = grad_w2.reshape(w2.value.shape)\n",
    "grad_b2_rs = grad_b2.reshape(b2.value.shape)\n",
    "\n",
    "ic(grad_w2_rs)\n",
    "ic(grad_b2_rs)\n",
    "print(\"For comparison of torches results see section Comparison to torch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce921d3-14bf-4517-b4aa-5ec8b85dfa76",
   "metadata": {},
   "source": [
    "### Comparison to $\\texttt{torch}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b208f02-9a06-4dd2-8d76-6cea60fa342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91bc428-79a2-4524-aca7-9d084501a2e0",
   "metadata": {},
   "source": [
    "### One-Dimensional and Single-Layer Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fe39420-de53-48c5-beae-519555cc4d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| grad_ws: array([0.30121278])\n",
      "ic| grad_bs: array([0.42685234])\n",
      "ic| ws_t.grad: tensor([0.3012], dtype=torch.float64)\n",
      "ic| bs_t.grad: tensor([0.4269], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 4 (scalar)\n",
      "torch gradient values for w and b: \n",
      " w: \n",
      " tensor([0.3012], dtype=torch.float64) \n",
      " b: \n",
      " tensor([0.4269], dtype=torch.float64) \n",
      "\n",
      "compared to gradient results of own algorithm: \n",
      " w :\n",
      " [0.30121278] \n",
      " b: \n",
      " [0.42685234]\n"
     ]
    }
   ],
   "source": [
    "# Same code as above, but the random parameters need to be in the same cell for comparison\n",
    "# Everything inside the lined box is a repetition, skip to torch code\n",
    "# -------------------------------------------------------------------------------------------\n",
    "# actual values\n",
    "xs_value = np.random.rand(1)\n",
    "ws_value = np.random.rand(1)\n",
    "bs_value = np.random.rand(1)\n",
    "func4s_target_value = np.random.rand(1)\n",
    "\n",
    "ws.grad_value = np.float64(0)\n",
    "xs.grad_value = np.float64(0)\n",
    "bs.grad_value = np.float64(0)\n",
    "\n",
    "ws = Expr_end_node(ws_value)\n",
    "xs = Expr_end_node(xs_value)\n",
    "bs = Expr_end_node(bs_value)\n",
    "func4s_target = Expr_end_node(func4s_target_value)\n",
    "\n",
    "tanh = Tanh()\n",
    "add = Add()\n",
    "multiply = Multiply()\n",
    "\n",
    "# defining the function\n",
    "func4s = tanh(add(multiply(ws,xs) , bs)) # = function 4\n",
    "\n",
    "# prediction value for loss function\n",
    "func4s_predict = forward(func4s)\n",
    "\n",
    "ws.grad_value = np.float64(0)\n",
    "xs.grad_value = np.float64(0)\n",
    "bs.grad_value = np.float64(0)\n",
    "backward(func4s)\n",
    "\n",
    "grad_ws = loss_backwards(func4s_predict, func4s_target.value)*ws.grad_value\n",
    "grad_xs = loss_backwards(func4s_predict, func4s_target.value)*xs.grad_value\n",
    "grad_bs = loss_backwards(func4s_predict, func4s_target.value)*bs.grad_value\n",
    "\n",
    "# -------------------------------------------------------------------------------------------\n",
    "# here starts the torch code\n",
    "def forward_function(x, w, b):\n",
    "     return torch.tanh(w * x + b)\n",
    "\n",
    "# torch expression for parameters\n",
    "xs_t = torch.tensor(xs_value, requires_grad=True)\n",
    "ws_t = torch.tensor(ws_value, requires_grad=True)\n",
    "bs_t = torch.tensor(bs_value, requires_grad=True)\n",
    "\n",
    "\n",
    "func4s_target_t = torch.tensor(func4s_target_value)\n",
    "\n",
    "func4s_predict_t = forward_function(xs_t, ws_t, bs_t,)\n",
    "\n",
    "loss_t = loss_function(func4s_predict_t, func4s_target_t)\n",
    "\n",
    "loss_t.backward()\n",
    "\n",
    "ic(grad_ws)\n",
    "ic(grad_bs)\n",
    "ic(ws_t.grad)\n",
    "ic(bs_t.grad)\n",
    "\n",
    "\n",
    "# Result\n",
    "print(\"Function 4 (scalar)\")\n",
    "print(\"torch gradient values for w and b: \\n\",\n",
    "      \"w: \\n\", ws_t.grad, \"\\n\",\n",
    "      \"b: \\n\", bs_t.grad, \"\\n\")\n",
    "print(\"compared to gradient results of own algorithm: \\n\", \n",
    "      \"w :\\n\", grad_ws, \"\\n\",\n",
    "      \"b: \\n\", grad_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52bf3f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| grad_w: array([-0.00054707, -0.00171534,  0.03409697,  0.10691214, -0.13023268,\n",
      "                   -0.40834868])\n",
      "ic| grad_b: array([-0.0023075 ,  0.14381925, -0.54931464])\n",
      "ic| w_t.grad: tensor([[-0.0005, -0.0017],\n",
      "                      [ 0.0341,  0.1069],\n",
      "                      [-0.1302, -0.4083]], dtype=torch.float64)\n",
      "ic| b_t.grad: tensor([-0.0023,  0.1438, -0.5493], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 4\n",
      "\n",
      " Input: \n",
      " w: \n",
      " [[0.90602038 0.27251732]\n",
      " [0.47833065 0.50656291]\n",
      " [0.86829964 0.3025354 ]] \n",
      " b: \n",
      " [0.82966095 0.02442584 0.2357855 ] \n",
      "\n",
      "neuron parameters: \n",
      " w: \n",
      " [[-0.00054707 -0.00171534]\n",
      " [ 0.03409697  0.10691214]\n",
      " [-0.13023268 -0.40834868]] \n",
      " b: \n",
      " [-0.0023075   0.14381925 -0.54931464] \n",
      "\n",
      "compared to gradient results of own algorithm: \n",
      " w :\n",
      " tensor([[-0.0005, -0.0017],\n",
      "        [ 0.0341,  0.1069],\n",
      "        [-0.1302, -0.4083]], dtype=torch.float64) \n",
      " b: \n",
      " tensor([-0.0023,  0.1438, -0.5493], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Same code as above, but the random parameters need to be in the same cell for comparison\n",
    "# Everything inside the lined box is a repetition, skip to torch code\n",
    "# -------------------------------------------------------------------------------------------\n",
    "\n",
    "# multidimensional values\n",
    "# multidimensional values\n",
    "w_value = np.random.rand(3,2)\n",
    "xv_value = np.random.rand(2)\n",
    "b_value = np.random.rand(3)\n",
    "func4_target_value = np.random.rand(3)\n",
    "\n",
    "w = Expr_end_node(w_value)\n",
    "xv = Expr_end_node(xv_value)\n",
    "b = Expr_end_node(b_value)\n",
    "func4_target = Expr_end_node(func4_target_value)\n",
    "\n",
    "tanh = Tanh()\n",
    "vecadd = Vector_vector_sum()\n",
    "matmul = Matrix_vector_product()\n",
    "\n",
    "# defining the function\n",
    "activation = Matrix_w_x_b()\n",
    "func4 = tanh(activation(w,xv,b)) # = function 4\n",
    "\n",
    "# prediction value for loss function\n",
    "func4_predict = forward(func4)\n",
    "\n",
    "w.grad_value = np.float64(0)\n",
    "xv.grad_value = np.float64(0)\n",
    "b.grad_value = np.float64(0)\n",
    "backward(func4)\n",
    "\n",
    "grad_w = loss_backwards(func4_predict, func4_target.value)@w.grad_value\n",
    "grad_xv = loss_backwards(func4_predict, func4_target.value)@xv.grad_value\n",
    "grad_b = loss_backwards(func4_predict, func4_target.value)@b.grad_value\n",
    "\n",
    "# reshaping the derivatives to the resepective parameter\n",
    "grad_w_rs = grad_w.reshape(w.value.shape)\n",
    "grad_xv_rs = grad_xv.reshape(xv.value.shape)\n",
    "grad_b_rs = grad_b.reshape(b.value.shape)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------\n",
    "# here starts the torch code\n",
    "\n",
    "def forward_function(x, w, b):\n",
    "     return torch.tanh(w @ x + b)\n",
    "\n",
    "# torch expression for parameters\n",
    "xv_t = torch.tensor(xv_value, requires_grad=True)\n",
    "w_t = torch.tensor(w_value, requires_grad=True)\n",
    "b_t = torch.tensor(b_value, requires_grad=True)\n",
    "\n",
    "\n",
    "func4_target_t = torch.tensor(func4_target_value)\n",
    "\n",
    "func4_predict_t = forward_function(xv_t, w_t, b_t)\n",
    "loss_t = loss_function(func4_predict_t, func4_target_t)\n",
    "\n",
    "loss_t.backward()\n",
    "\n",
    "ic(grad_w)\n",
    "ic(grad_b)\n",
    "ic(w_t.grad)\n",
    "ic(b_t.grad)\n",
    "\n",
    "print(\"Function 4\")\n",
    "print(\"\\n Input: \\n\",\n",
    "      \"w: \\n\", w_value, \"\\n\",\n",
    "      \"b: \\n\", b_value , \"\\n\")\n",
    "\n",
    "print(\"neuron parameters: \\n\",\n",
    "      \"w: \\n\", grad_w_rs, \"\\n\",\n",
    "      \"b: \\n\", grad_b_rs, \"\\n\")\n",
    "print(\"compared to gradient results of own algorithm: \\n\", \n",
    "      \"w :\\n\", w_t.grad, \"\\n\",\n",
    "      \"b: \\n\", b_t.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952366a-5a0e-4e31-a348-475369be10fd",
   "metadata": {},
   "source": [
    "### Multidimensional and multilayer comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f3af9b4-44f8-45cb-a7fe-836cc07ca1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| grad_w2_rs: array([[0.16946913, 0.24740022, 0.24647518],\n",
      "                       [0.03214457, 0.04692638, 0.04675093],\n",
      "                       [0.20031027, 0.29242379, 0.29133041]])\n",
      "ic| grad_b2_rs: array([0.28919791, 0.05485449, 0.34182811])\n",
      "ic| w2_t.grad: tensor([[0.1695, 0.2474, 0.2465],\n",
      "                       [0.0321, 0.0469, 0.0468],\n",
      "                       [0.2003, 0.2924, 0.2913]], dtype=torch.float64)\n",
      "ic| b2_t.grad: tensor([0.2892, 0.0549, 0.3418], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 4\n",
      "\n",
      " Input layer: \n",
      " w: \n",
      " [[0.11067609 0.74409225]\n",
      " [0.95022696 0.63816965]\n",
      " [0.19433155 0.84210456]] \n",
      " b: \n",
      " [0.2384311  0.39252483 0.73286122]\n",
      "\n",
      " Second layer:\n",
      "torch values for w and b: \n",
      " w: \n",
      " tensor([[0.1443, 0.1188],\n",
      "        [0.0266, 0.0219],\n",
      "        [0.0405, 0.0333]], dtype=torch.float64) \n",
      " b: \n",
      " tensor([0.2410, 0.0445, 0.0676], dtype=torch.float64) \n",
      "\n",
      "compared to gradient results of own algorithm: \n",
      " w :\n",
      " [[0.14432145 0.11880729]\n",
      " [0.02663425 0.02192566]\n",
      " [0.04048723 0.03332961]] \n",
      " b: \n",
      " [0.2409894  0.04447414 0.06760599]\n",
      "\n",
      " Third layer:\n",
      "torch values for w and b: \n",
      " w: \n",
      " tensor([[0.1695, 0.2474, 0.2465],\n",
      "        [0.0321, 0.0469, 0.0468],\n",
      "        [0.2003, 0.2924, 0.2913]], dtype=torch.float64) \n",
      " b: \n",
      " tensor([0.2892, 0.0549, 0.3418], dtype=torch.float64) \n",
      "\n",
      "compared to manual derivatives through chain rule: \n",
      " w :\n",
      " [[0.16946913 0.24740022 0.24647518]\n",
      " [0.03214457 0.04692638 0.04675093]\n",
      " [0.20031027 0.29242379 0.29133041]] \n",
      " b: \n",
      " [0.28919791 0.05485449 0.34182811]\n"
     ]
    }
   ],
   "source": [
    "# Same code as above, but the random parameters need to be in the same cell for comparison\n",
    "# Everything inside the lined box is a repetition, skip to torch code\n",
    "# -------------------------------------------------------------------------------------------\n",
    "# actual values\n",
    "w_value = np.random.rand(3,2)\n",
    "xv_value = np.random.rand(2)\n",
    "b_value = np.random.rand(3)\n",
    "\n",
    "w2_value = np.random.rand(3,3)\n",
    "b2_value = np.random.rand(3)\n",
    "func4_2_target_value = np.random.rand(3)\n",
    "\n",
    "# node expression\n",
    "w = Expr_end_node(w_value)\n",
    "xv = Expr_end_node(xv_value)\n",
    "b = Expr_end_node(b_value)\n",
    "\n",
    "w2 = Expr_end_node(w2_value)\n",
    "b2 = Expr_end_node(b2_value)\n",
    "\n",
    "\n",
    "func4_2_target = Expr_end_node(func4_2_target_value)\n",
    "\n",
    "\n",
    "tanh = Tanh()\n",
    "vecadd = Vector_vector_sum()\n",
    "matmul = Matrix_vector_product()\n",
    "activation = Matrix_w_x_b()\n",
    "\n",
    "func4 = tanh(activation(w,xv,b))\n",
    "func4_2 = tanh(activation(w2, func4, b2))\n",
    "func4_2_predict = forward(func4_2)\n",
    "\n",
    "w.grad_value = np.float64(0)\n",
    "xv.grad_value = np.float64(0)\n",
    "b.grad_value = np.float64(0)\n",
    "w2.grad_value = np.float64(0)\n",
    "b2.grad_value = np.float64(0)\n",
    "backward(func4_2)\n",
    "\n",
    "grad_w = loss_backwards(func4_2_predict, func4_2_target.value)@w.grad_value\n",
    "grad_b = loss_backwards(func4_2_predict, func4_2_target.value)@b.grad_value\n",
    "\n",
    "grad_w2 = loss_backwards(func4_2_predict, func4_2_target.value)@w2.grad_value\n",
    "grad_b2 = loss_backwards(func4_2_predict, func4_2_target.value)@b2.grad_value\n",
    "\n",
    "# reshaping the derivatives to the resepective parameter\n",
    "grad_w_rs = grad_w.reshape(w.value.shape)\n",
    "grad_b_rs = grad_b.reshape(b.value.shape)\n",
    "grad_w2_rs = grad_w2.reshape(w2.value.shape)\n",
    "grad_b2_rs = grad_b2.reshape(b2.value.shape)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------\n",
    "# here starts the torch code\n",
    "def forward_function(x, w1, b1, w2, b2):\n",
    "     return torch.tanh(w2 @ torch.tanh(w1 @ x + b1) + b2)\n",
    "\n",
    "# torch expression for parameters\n",
    "xv_t = torch.tensor(xv_value, requires_grad=True)\n",
    "w_t = torch.tensor(w_value, requires_grad=True)\n",
    "b_t = torch.tensor(b_value, requires_grad=True)\n",
    "\n",
    "w2_t = torch.tensor(w2_value, requires_grad=True)\n",
    "b2_t = torch.tensor(b2_value, requires_grad=True)\n",
    "\n",
    "\n",
    "func4_2_target_t = torch.tensor(func4_2_target_value)\n",
    "\n",
    "func4_2_predict_t = forward_function(xv_t, w_t, b_t, w2_t, b2_t)\n",
    "loss_t = loss_function(func4_2_predict_t, func4_2_target_t)\n",
    "\n",
    "loss_t.backward()\n",
    "\n",
    "ic(grad_w2_rs)\n",
    "ic(grad_b2_rs)\n",
    "ic(w2_t.grad)\n",
    "ic(b2_t.grad)\n",
    "\n",
    "print(\"Function 4\")\n",
    "print(\"\\n Input layer: \\n\",\n",
    "      \"w: \\n\", w_value, \"\\n\",\n",
    "      \"b: \\n\", b_value )\n",
    "print(\"\\n Second layer:\")\n",
    "print(\"torch values for w and b: \\n\",\n",
    "      \"w: \\n\", w_t.grad, \"\\n\",\n",
    "      \"b: \\n\", b_t.grad, \"\\n\")\n",
    "print(\"compared to gradient results of own algorithm: \\n\", \n",
    "      \"w :\\n\", grad_w_rs, \"\\n\",\n",
    "      \"b: \\n\", grad_b_rs)\n",
    "print(\"\\n Third layer:\")\n",
    "print(\"torch values for w and b: \\n\",\n",
    "      \"w: \\n\", w2_t.grad, \"\\n\",\n",
    "      \"b: \\n\", b2_t.grad, \"\\n\")\n",
    "print(\"compared to manual derivatives through chain rule: \\n\", \n",
    "      \"w :\\n\", grad_w2_rs, \"\\n\",\n",
    "      \"b: \\n\", grad_b2_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3dfa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
