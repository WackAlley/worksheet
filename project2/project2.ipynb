{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "07d7a675-fc31-4a1a-a56f-f7128802c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f6ed9-4711-4b53-8ec4-b5b6bf682edd",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "- $\\texttt{forward\\_backward\\_functions\\_and\\_nodes}$: this library contains three things: \n",
    "    - The mathematical operators are defined as classes\n",
    "    - The nodes of a node tree are defined by their behavior: variables/endnodes are defined as instances of $\\texttt{Expr\\_end\\_node()}$, while functions are defined as instances of $\\texttt{Expr\\_node()}$. \n",
    "    - ABC-Function: Each time a class is called, it is defined as an instance of $\\texttt{Expr\\_node()}$. This means that calling an operator class is enough for the propagation algorithm to understand what and where the nodes are. \n",
    "- $\\texttt{print\\_graph}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e0e12b73-3264-48c8-abe4-a9d3dab1702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forward_backward_functions_and_nodes import * \n",
    "from print_graph import print_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af883d-fb84-4c7d-9503-d86bfd890234",
   "metadata": {},
   "source": [
    "# Propagation algorithms\n",
    "The forward pass gives the computated value of the specified node. If the node is not an endnode, it will keep recurring itself until it reaches one. \\\n",
    "The backward propagation is a recursive function that will record the argument of the outer derivative, compute the outer derivative, and mulitplty it with the derivative of the argument. The derivative of the argument is done the same way. The algorithm will keep doing this unit it reaches an endnode, where it will assign the value of the endnode with the total accumulated product. \\\n",
    "The possibility of multiple arguments is considered with the $\\texttt{if len(node.childs) == 1}$ - clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "054932a3-ff42-4ef3-bad1-7591b1f0e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(node):\n",
    "    return node.forward_func(*(forward(child) for child in node.childs)) if type(node) is not Expr_end_node else node.value \n",
    "    \n",
    "# def backward(node, value = np.float64(1)):\n",
    "#     if type(node) is not Expr_end_node:\n",
    "#         child_values = [forward(child) for child in node.childs] # computes the argument of the outer derivative. In other words: it computes g of f'(g)\n",
    "#         if len(node.childs) == 1:\n",
    "#             new_value = node.backward_func(*child_values) # computes the outer derivative f'(g)\n",
    "#             if value.ndim == 0 or new_value.ndim == 0:\n",
    "#                 backward(node.childs[0], value * new_value)\n",
    "#             else: \n",
    "#                 backward(node.childs[0], new_value.T @ value) # @ is matrix product\n",
    "#         else:\n",
    "#             for child, new_value in zip(node.childs, node.backward_func(*child_values), strict=True):\n",
    "#                 if value.ndim == 0 or new_value.ndim == 0:\n",
    "#                     backward(child, value * new_value)\n",
    "#                 else: \n",
    "#                     backward(child, new_value.T @ value)                 \n",
    "#     else:\n",
    "#         node.grad_value += value\n",
    "\n",
    "\n",
    "def backward(node, value = np.float64(1)):\n",
    "    if type(node) is not Expr_end_node:\n",
    "        child_values = [forward(child) for child in node.childs]\n",
    "        if len(node.childs) == 1:\n",
    "            # product of inner derivatives\n",
    "            # value =parant_node derivative(all of parant's childs feeded forward)\n",
    "            # new_value = derivative_of_current_node(all of current cilds feeded forward)\n",
    "            new_value = node.backward_func(*child_values)\n",
    "            if value.ndim == 0 or new_value.ndim == 0:\n",
    "                backward(node.childs[0], value * new_value)\n",
    "            else: \n",
    "                backward(node.childs[0], value @ new_value) # @ is matrix product\n",
    "            \n",
    "        else:\n",
    "            for child, new_value in zip(node.childs, node.backward_func(*child_values), strict=True):\n",
    "                if value.ndim == 0 or new_value.ndim == 0:\n",
    "                    backward(child, value * new_value)\n",
    "                else: \n",
    "                    backward(child, value @ new_value ) # @ is matrix product                  \n",
    "    else: \n",
    "        node.grad_value += value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2441ffd-91ff-4641-af26-1dff80fb85b6",
   "metadata": {},
   "source": [
    "# Example functions\n",
    "\n",
    "The following cells are structured as follows: \n",
    "- Each cell contains one example function. \\\n",
    "For each function, we define the operators, parameters, and the function itself. The operators are defined as their respective operator classes. \\\n",
    "Example: \n",
    "| mathematical operator | operator class  |\n",
    "| --- | --- |\n",
    "| + | Add() |\n",
    "| $\\cdot$ | Multiply()|\n",
    "| sin() | Sin() | \n",
    "\n",
    "- This function will then be depicted as a node tree via the $\\texttt{print\\_graph}$ function.\n",
    "- Finally, we perform the forward and backward propagation. The values of the propagations will be compared with values of the analytically solven function and derivative(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e505c03c-514b-4ad7-8a06-1ddb44b6a65b",
   "metadata": {},
   "source": [
    "## Parameters and mathematical operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "de80b1fe-e59f-4aac-85da-40ef541cc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Expr_end_node(np.random.rand(1))\n",
    "x2 = Expr_end_node(np.random.rand(1))\n",
    "x = Expr_end_node(np.random.rand(1))\n",
    "w = Expr_end_node(np.random.rand(3,3))\n",
    "xv = Expr_end_node(np.random.rand(3))\n",
    "b = Expr_end_node(np.random.rand(3))\n",
    "\n",
    "add = Add()\n",
    "add_2 = Add_scalar(2)\n",
    "multiply = Multiply()\n",
    "multiply_3 = Multiply_scalar(3)\n",
    "multiply_4 = Multiply_scalar(4)\n",
    "sin = Sin()\n",
    "log = Log()\n",
    "tanh = Tanh()\n",
    "vecadd = Vector_vector_sum()\n",
    "matmul = Matrix_vector_product()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc2b3b6-96d4-4bf5-a765-fe76cfe7a219",
   "metadata": {},
   "source": [
    "## Function 1: $f(x_1,x_2) = \\log(x_1 \\cdot x_2) \\cdot \\sin(x_2) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "99f579f8-ac29-4da5-9fe7-e63634a8244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| forward(func1): array([-0.78560797])\n",
      "ic| np.log(x1.value * x2.value) * np.sin(x2.value): array([-0.78560797])\n",
      "ic| x1.grad_value: array([1.29875479])\n",
      "ic| np.sin(x2.value) / x1.value: array([1.29875479])\n",
      "ic| x2.grad_value: array([-1.66175517])\n",
      "ic| np.sin(x2.value) / x2.value + np.log(x1.value * x2.value) * np.cos(x2.value): array([-1.66175517])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 1\n",
      "Calculus values of x1 derivative and x2 derivative: [1.29875479] [-1.66175517]\n",
      "Compared to derivatives through chain rule:         [1.29875479] [-1.66175517]\n"
     ]
    }
   ],
   "source": [
    "# defining the function\n",
    "func1 = multiply(log(multiply(x1, x2)), sin(x2))\n",
    "\n",
    "# graphical depiction\n",
    "graph1 = graphviz.Digraph('graph1', comment='test') \n",
    "graph1.attr(rankdir=\"LR\")\n",
    "print_graph(func1, graph1)\n",
    "graph1.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "\n",
    "# analytical function and its derivative(s)\n",
    "mfunc1 = np.log(x1.value * x2.value) * np.sin(x2.value) # analytical function\n",
    "mdfunc1dx1 = np.sin(x2.value) / x1.value # analytical derivative w.r.t. x1 \n",
    "mdfunc1dx2 = np.sin(x2.value) / x2.value + np.log(x1.value * x2.value) * np.cos(x2.value) # analytical derivative w.r.t. x2\n",
    "\n",
    "# comparison to analytical value\n",
    "# comparison 1: forward propagation\n",
    "ic(forward(func1)) # value of the function via forward propagation\n",
    "ic(np.log(x1.value * x2.value) * np.sin(x2.value)) # value of the analytical function\n",
    "\n",
    "# comparison 2: backward propagation\n",
    "# as we reuse the same parameter names for multiple functions, we set the derivatives w.r.t. the parameters to zero, before performing the derivatives.\n",
    "x1.grad_value=0\n",
    "x2.grad_value=0\n",
    "backward(func1) # performing the derivative via backward propagation\n",
    "ic(x1.grad_value) # value of the derivative w.r.t. x1 via backward propagation\n",
    "ic(np.sin(x2.value) / x1.value) # value of the analytical derivative w.r.t. x1 \n",
    "ic(x2.grad_value) # value of the derivative w.r.t. x2 via backward propagation\n",
    "ic(np.sin(x2.value) / x2.value + np.log(x1.value * x2.value) * np.cos(x2.value)) # value of the analytical derivative w.r.t. x2\n",
    "\n",
    "\n",
    "# Result\n",
    "print(\"function 1\")\n",
    "print(\"Calculus values of x1 derivative and x2 derivative:\", mdfunc1dx1, mdfunc1dx2 )\n",
    "print(\"Compared to derivatives through chain rule:        \", x1.grad_value, x2.grad_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f7dd8-e86c-49d9-b29f-1c4f34845236",
   "metadata": {},
   "source": [
    "## Function 2: $g(x_1, x_2) = x_1 \\cdot x_2 (x_1 + x_2) $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "dbe59784-a76f-4e17-a37a-76d8ae6655a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| forward(func2): array([0.03205012])\n",
      "ic| x1.value * x2.value * (x1.value + x2.value): array([0.03205012])\n",
      "ic| x1.grad_value: array([0.20950889])\n",
      "ic| mdfunc2dx1: array([0.20950889])\n",
      "ic| x2.grad_value: array([0.17428908])\n",
      "ic| mdfunc2dx2: array([0.17428908])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 2\n",
      "Calculus values of x1 derivative and x2 derivative: [0.20950889] [0.17428908]\n",
      "Compared to derivatives through chain rule:         [0.20950889] [0.17428908]\n"
     ]
    }
   ],
   "source": [
    "# defining the function\n",
    "func2 = multiply(x1, multiply(x2, add(x1, x2)))\n",
    "\n",
    "# graphical depiction\n",
    "graph2 = graphviz.Digraph('graph2', comment='test') \n",
    "graph2.attr(rankdir=\"LR\")\n",
    "print_graph(func2, graph2)\n",
    "graph2.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "# analytical function and its derivative(s)\n",
    "mfunc2 = x1.value * x2.value * (x1.value + x2.value)\n",
    "mdfunc2dx1 = x2.value * (x1.value + x2.value) + x1.value * x2.value\n",
    "mdfunc2dx2 = x1.value * (x1.value + x2.value) + x1.value * x2.value\n",
    "\n",
    "# comparison\n",
    "ic(forward(func2))\n",
    "ic(x1.value * x2.value * (x1.value + x2.value))\n",
    "\n",
    "x1.grad_value=0\n",
    "x2.grad_value=0\n",
    "backward(func2)\n",
    "ic(x1.grad_value)\n",
    "ic(mdfunc2dx1)\n",
    "ic(x2.grad_value)\n",
    "ic(mdfunc2dx2)\n",
    "\n",
    "# result\n",
    "print(\"Function 2\")\n",
    "print(\"Calculus values of x1 derivative and x2 derivative:\", mdfunc2dx1, mdfunc2dx2 )\n",
    "print(\"Compared to derivatives through chain rule:        \", x1.grad_value, x2.grad_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de858bf-d98b-4bd3-ad5d-1e63e2afc067",
   "metadata": {},
   "source": [
    "## Function 3: $h(x) = 3x^2 + 4x + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ccb60fa2-89da-453d-9d58-65b3371e3041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| forward(func3): array([2.12826373])\n",
      "ic| 3*x.value**2 + 4 * x.value + 2: array([2.12826373])\n",
      "ic| mdfunc3dx: array([4.18797859])\n",
      "ic| x.grad_value: array([4.18797859])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 3\n",
      "Calculus values of x derivative:                    [4.18797859]\n",
      "Compared to derivatives through chain rule:         [4.18797859]\n"
     ]
    }
   ],
   "source": [
    "# defining the function\n",
    "func3 = add_2( add( multiply_3(multiply(x,x)) , multiply_4(x) ))\n",
    "\n",
    "# graphical depiction\n",
    "graph3 = graphviz.Digraph('graph3', comment='test') \n",
    "graph3.attr(rankdir=\"LR\")\n",
    "print_graph(func3, graph3)\n",
    "graph3.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "# comparison\n",
    "mfunc3 = 3*x.value**2 + 4 * x.value + 2\n",
    "mdfunc3dx = 6 * x.value + 4\n",
    "\n",
    "ic(forward(func3))\n",
    "ic(3*x.value**2 + 4 * x.value + 2)\n",
    "\n",
    "x.grad_value = 0\n",
    "backward(func3)\n",
    "ic(mdfunc3dx)\n",
    "ic(x.grad_value)\n",
    "\n",
    "\n",
    "# Result\n",
    "print(\"Function 3\")\n",
    "print(\"Calculus values of x derivative:                   \", mdfunc3dx)\n",
    "print(\"Compared to derivatives through chain rule:        \", x.grad_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70448188-cd85-4e37-8388-c663505fd4ff",
   "metadata": {},
   "source": [
    "## Function 4: Neuron$(\\vec x, w, \\vec b) = \\tanh(w\\cdot \\vec x + \\vec b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dd5f63-2b52-4171-a6a8-c7834787e4ec",
   "metadata": {},
   "source": [
    "### Manual algorithm\n",
    "For the neuron activation function, we will compare the derivatives with what $\\texttt{torch}$ computes. \\\n",
    "The backpropagation will include the loss function as the outermost derivative, so that we have\n",
    "$$\n",
    " \\frac{\\partial \\text{Loss}(\\vec y)}{\\partial \\vartheta_{ij}} = \\frac{\\partial \\text{Loss}}{\\partial \\vec y}  \\frac{\\partial \\vec y}{\\partial \\vartheta_{ij}}\n",
    "$$\n",
    "with $\\vartheta_{ij}$ either being the weight matrix $w_{ij}$ or the bias vector $\\vec b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f98c7003-6e8b-4030-8f9d-bc1686db4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function   \n",
    "def loss_function(y_pred, y_target):\n",
    "     return (y_pred - y_target).pow(2).sum()\n",
    "    \n",
    "# (d loss)/(d y_pred) = 2*(y_pred - y_target)\n",
    "def loss_backwards(y_pred, y_target):\n",
    "     return 2*(y_pred - y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b3d47224-5f39-4954-9200-2c5aaa427f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| grad_w_cs: array([[ 0.17129803,  0.13099558],\n",
      "                      [ 0.46455712,  0.35525761],\n",
      "                      [-0.00722303, -0.00552362]])\n",
      "ic| grad_xv_cs: array([0.40642053, 0.58518156])\n",
      "ic| grad_b_cs: array([ 0.19586661,  0.53118667, -0.008259  ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 4\n",
      "Derivatives through chain rule:        \n",
      " w :\n",
      " [[ 0.17129803  0.13099558]\n",
      " [ 0.46455712  0.35525761]\n",
      " [-0.00722303 -0.00552362]] \n",
      " xv: \n",
      " [0.40642053 0.58518156] \n",
      " b: \n",
      " [ 0.19586661  0.53118667 -0.008259  ]\n"
     ]
    }
   ],
   "source": [
    "w_value = np.random.rand(3,2)\n",
    "xv_value = np.random.rand(2)\n",
    "b_value = np.random.rand(3)\n",
    "func4_target_value = np.random.rand(3)\n",
    "\n",
    "w = Expr_end_node(w_value)\n",
    "xv = Expr_end_node(xv_value)\n",
    "b = Expr_end_node(b_value)\n",
    "func4_target = Expr_end_node(func4_target_value)\n",
    "\n",
    "tanh = Tanh()\n",
    "vecadd = Vector_vector_sum()\n",
    "matmul = Matrix_vector_product()\n",
    "\n",
    "# defining the function\n",
    "activation = Matrix_w_x_b()\n",
    "func4 = tanh(activation(w,xv,b)) # = function 4\n",
    "\n",
    "# prediction value for loss function\n",
    "func4_predict = forward(func4)\n",
    "\n",
    "# graphical depiction\n",
    "graph4 = graphviz.Digraph('graph4', comment='test') \n",
    "graph4.attr(rankdir=\"LR\")\n",
    "print_graph(func4, graph4)\n",
    "graph4.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "\n",
    "w.grad_value = np.float64(0)\n",
    "xv.grad_value = np.float64(0)\n",
    "b.grad_value = np.float64(0)\n",
    "backward(func4)\n",
    "\n",
    "grad_w = loss_backwards(func4_predict, func4_target.value)@w.grad_value\n",
    "grad_xv = loss_backwards(func4_predict, func4_target.value)@xv.grad_value\n",
    "grad_b = loss_backwards(func4_predict, func4_target.value)@b.grad_value\n",
    "\n",
    "# reshaping the derivatives to the resepective parameter\n",
    "grad_w_cs = grad_w.reshape(w.value.shape)\n",
    "grad_xv_cs = grad_xv.reshape(xv.value.shape)\n",
    "grad_b_cs = grad_b.reshape(b.value.shape)\n",
    "\n",
    "ic(grad_w_cs)\n",
    "ic(grad_xv_cs)\n",
    "ic(grad_b_cs)\n",
    "\n",
    "\n",
    "# Result\n",
    "print(\"Function 4\")\n",
    "print(\"Derivatives through chain rule:        \\n\", \"w :\\n\",\n",
    "      grad_w_cs, \"\\n xv: \\n\",\n",
    "      grad_xv_cs, \"\\n b: \\n\", \n",
    "      grad_b_cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d13cb6-0531-4db1-83fa-0f48f5f2726d",
   "metadata": {},
   "source": [
    "### Function 4 extension: multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0ffd5a03-8a83-4150-bb26-8b65146d70f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| grad_w2_rs: array([[0.03194891, 0.02842431, 0.0328905 ],\n",
      "                       [0.16102504, 0.14326079, 0.16577069],\n",
      "                       [0.12138001, 0.10798939, 0.12495726]])\n",
      "ic| grad_b2_rs: array([0.03484134, 0.1756031 , 0.13236889])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.03484134, 0.1756031 , 0.13236889])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2_value = np.random.rand(3,3)\n",
    "b2_value = np.random.rand(3)\n",
    "func4_2_target_value = np.random.rand(3)\n",
    "\n",
    "w2 = Expr_end_node(w2_value)\n",
    "b2 = Expr_end_node(b2_value)\n",
    "func4_2_target = Expr_end_node(func4_2_target_value)\n",
    "\n",
    "# second neuron\n",
    "func4_2 = tanh(activation(w2, func4, b2))\n",
    "\n",
    "# graphical depiction\n",
    "graph4_2 = graphviz.Digraph('graph4.2', comment='test') \n",
    "graph4_2.attr(rankdir=\"LR\")\n",
    "print_graph(func4_2, graph4_2)\n",
    "graph4_2.render(directory='graph_out/tt', view=True)\n",
    "\n",
    "w2.grad_value = np.float64(0)\n",
    "b2.grad_value = np.float64(0)\n",
    "w.grad_value = np.float64(0)\n",
    "b.grad_value = np.float64(0)\n",
    "backward(func4_2)\n",
    "\n",
    "grad_w2 = loss_backwards(func4_2_predict, func4_2_target.value)@w2.grad_value\n",
    "grad_b2 = loss_backwards(func4_2_predict, func4_2_target.value)@b2.grad_value\n",
    "\n",
    "# reshaping the derivatives to the resepective parameter\n",
    "grad_w2_rs = grad_w2.reshape(w2.value.shape)\n",
    "grad_b2_rs = grad_b2.reshape(b2.value.shape)\n",
    "\n",
    "ic(grad_w2_rs)\n",
    "ic(grad_b2_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce921d3-14bf-4517-b4aa-5ec8b85dfa76",
   "metadata": {},
   "source": [
    "### Comparison to $\\texttt{torch}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1b208f02-9a06-4dd2-8d76-6cea60fa342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6f3af9b4-44f8-45cb-a7fe-836cc07ca1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| grad_w2_rs: array([[0.03448134, 0.03612454, 0.03667722],\n",
      "                       [0.22805968, 0.23892778, 0.2425832 ],\n",
      "                       [0.25727241, 0.26953263, 0.27365628]])\n",
      "ic| grad_b2_rs: array([0.03730443, 0.24673157, 0.27833603])\n",
      "ic| w2_t.grad: tensor([[0.0345, 0.0361, 0.0367],\n",
      "                       [0.2281, 0.2389, 0.2426],\n",
      "                       [0.2573, 0.2695, 0.2737]], dtype=torch.float64)\n",
      "ic| b2_t.grad: tensor([0.0373, 0.2467, 0.2783], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 4\n",
      "torch values for w and b: \n",
      " w: \n",
      " tensor([[0.0345, 0.0361, 0.0367],\n",
      "        [0.2281, 0.2389, 0.2426],\n",
      "        [0.2573, 0.2695, 0.2737]], dtype=torch.float64) \n",
      " b: \n",
      " tensor([0.0373, 0.2467, 0.2783], dtype=torch.float64) \n",
      "\n",
      "compared to manual derivatives through chain rule: \n",
      " w :\n",
      " [[0.03448134 0.03612454 0.03667722]\n",
      " [0.22805968 0.23892778 0.2425832 ]\n",
      " [0.25727241 0.26953263 0.27365628]] \n",
      " b: \n",
      " [0.03730443 0.24673157 0.27833603]\n"
     ]
    }
   ],
   "source": [
    "# Same code as above, but the random parameters need to be in the same cell for comparison\n",
    "# Everything inside the lined box is a repetition, skip to torch code\n",
    "# -------------------------------------------------------------------------------------------\n",
    "# actual values\n",
    "w_value = np.random.rand(3,2)\n",
    "xv_value = np.random.rand(2)\n",
    "b_value = np.random.rand(3)\n",
    "\n",
    "w2_value = np.random.rand(3,3)\n",
    "b2_value = np.random.rand(3)\n",
    "func4_2_target_value = np.random.rand(3)\n",
    "\n",
    "# node expression\n",
    "w = Expr_end_node(w_value)\n",
    "xv = Expr_end_node(xv_value)\n",
    "b = Expr_end_node(b_value)\n",
    "\n",
    "w2 = Expr_end_node(w2_value)\n",
    "b2 = Expr_end_node(b2_value)\n",
    "\n",
    "func4_2_target = Expr_end_node(func4_2_target_value)\n",
    "\n",
    "\n",
    "tanh = Tanh()\n",
    "vecadd = Vector_vector_sum()\n",
    "matmul = Matrix_vector_product()\n",
    "activation = Matrix_w_x_b()\n",
    "\n",
    "func4 = tanh(activation(w,xv,b))\n",
    "func4_2 = tanh(activation(w2, func4, b2))\n",
    "func4_2_predict = forward(func4_2)\n",
    "\n",
    "w.grad_value = np.float64(0)\n",
    "xv.grad_value = np.float64(0)\n",
    "b.grad_value = np.float64(0)\n",
    "backward(func4_2)\n",
    "\n",
    "grad_w2 = loss_backwards(func4_2_predict, func4_2_target.value)@w2.grad_value\n",
    "grad_b2 = loss_backwards(func4_2_predict, func4_2_target.value)@b2.grad_value\n",
    "\n",
    "# reshaping the derivatives to the resepective parameter\n",
    "grad_w2_rs = grad_w2.reshape(w2.value.shape)\n",
    "grad_b2_rs = grad_b2.reshape(b2.value.shape)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------\n",
    "# here starts the torch code\n",
    "def forward_function(x, w1, b1, w2, b2):\n",
    "     return torch.tanh(w2 @ torch.tanh(w1 @ x + b1) + b2)\n",
    "\n",
    "# torch expression for parameters\n",
    "xv_t = torch.tensor(xv_value, requires_grad=True)\n",
    "w_t = torch.tensor(w_value, requires_grad=True)\n",
    "b_t = torch.tensor(b_value, requires_grad=True)\n",
    "\n",
    "w2_t = torch.tensor(w2_value, requires_grad=True)\n",
    "b2_t = torch.tensor(b2_value, requires_grad=True)\n",
    "\n",
    "func4_2_target_t = torch.tensor(func4_2_target_value)\n",
    "\n",
    "\n",
    "func4_2_predict_t = forward_function(xv_t, w_t, b_t, w2_t, b2_t)\n",
    "loss_t = loss_function(func4_2_predict_t, func4_2_target_t)\n",
    "\n",
    "loss_t.backward()\n",
    "\n",
    "ic(grad_w2_rs)\n",
    "ic(grad_b2_rs)\n",
    "ic(w2_t.grad)\n",
    "ic(b2_t.grad)\n",
    "\n",
    "print(\"Function 4\")\n",
    "print(\"torch values for w and b: \\n\",\n",
    "      \"w: \\n\", w2_t.grad, \"\\n\",\n",
    "      \"b: \\n\", b2_t.grad, \"\\n\")\n",
    "print(\"compared to manual derivatives through chain rule: \\n\", \n",
    "      \"w :\\n\", grad_w2_rs, \"\\n\",\n",
    "      \"b: \\n\", grad_b2_rs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
