{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83350876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from __future__ import annotations\n",
    "from numbers import Number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b8668e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| l.forward_func(2): -0.4161468365471424\n",
      "ic| 2492483270.py:111 in <module> at 11:35:55.519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.sin object at 0x7f1754eeb620> <ufunc 'sin'> <ufunc 'cos'>\n",
      "<ufunc 'sin'> <ufunc 'cos'>\n",
      "<ufunc 'multiply'>\n",
      "<bound method multiply.<lambda> of <__main__.multiply object at 0x7f1754c69950>>\n",
      "<ufunc 'multiply'>\n",
      "-0.4161468365471424 -0.4161468365471424\n",
      "<ufunc 'sin'> <ufunc 'cos'>\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from typing import Callable\n",
    "from typing import Type\n",
    "from typing import Tuple\n",
    "#from collections.abc import Iterable \n",
    "#@abstractmethod\n",
    "#    @property\n",
    "#    @abstractmethod\n",
    "# no need to call super\n",
    "from icecream import ic\n",
    "    \n",
    "    \n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "        \n",
    "class function(ABC):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def forward_func(self) -> Callable:\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def backward_func(self) -> Callable:\n",
    "        pass\n",
    "    \n",
    "    def get_functions(self) -> Tuple[Callable, Callable]:\n",
    "        return self.forward_func, self.backward_func\n",
    "    \n",
    "    def __call__(self, *inner) -> expr_node: # simplifies the syntax for chaning of functions\n",
    "        return expr_node(self, inner)\n",
    "        \n",
    "    \n",
    "\n",
    "class sin(function):\n",
    "    forward_func = np.sin\n",
    "    backward_func = np.cos\n",
    "\n",
    "class tanh(function):\n",
    "    forward_func = np.tanh\n",
    "    backward_func = lambda x: 1 - np.square(tanh(x))\n",
    "\n",
    "class cos(function):\n",
    "    forward_func = np.cos\n",
    "    backward_func = lambda x: -np.sin(x)\n",
    "          \n",
    "\n",
    "class multiply(function):\n",
    "    \n",
    "    forward_func = lambda: None # to be overwitten in constructor\n",
    "    backward_func = lambda: None # to be overwitten in constructor\n",
    "    \n",
    "    def __init__(self,*, allow_arbitrary_many = False): # * makes allow_arbitary_many to keword only\n",
    "        if not allow_arbitrary_many: # simple case two factors\n",
    "            self.forward_func = np.multiply\n",
    "            self.backward_func = lambda x, y: (y, x)\n",
    "        else:\n",
    "            forward_func = lambda *x: np.prod(np.vstack(x), axis=0)\n",
    "            self.backward_func = lambda *x: (np.prod(np.vstack(values), axis=0) for values in combinations(x, len(x) - 1))\n",
    "                   \n",
    "            \n",
    "        \n",
    "class add(function):\n",
    "    \n",
    "    forward_func = lambda: None # to be overwitten in constructor\n",
    "    backward_func = lambda: None # to be overwitten in constructor\n",
    "    \n",
    "    def __init__(self,*, allow_arbitrary_many = False): # new is clalled before the constructer (before: \"__init__\")\n",
    "        if not allow_arbitrary_many:\n",
    "            self.forward_func = np.add \n",
    "            self.backward_func = lambda x, y: (np.full_like(x, 1), np.full_like(y, 1)) #arrays with same shape of x and y, filled with 1\n",
    "        else:\n",
    "            self.forward_func = lambda *x: np.sum(np.vstack(x), axis=0)\n",
    "            self.backward_func = lambda *x: (np.full_like(x[0], 1) for _ in range(len(x))) #backward_func = lambda *x: (np.ones(len(x[0]) if isinstance(x[0], Iterable) else 1) for _ in range(len(x))))\n",
    "\n",
    "\n",
    "class tanh(function):\n",
    "\n",
    "    forward_func = np.tanh\n",
    "    backward_func = lambda x: 1/(cosh(x)**2)\n",
    "              \n",
    "    \n",
    "#b = function2(np.sin, np.cos)\n",
    "c = sin()\n",
    "print(c, c.forward_func, c.backward_func)\n",
    "f1, f2 = c.get_functions()\n",
    "print(f1, f2)\n",
    "#c.eval_forward(2)\n",
    "d = [sin(), sin()]\n",
    "\n",
    "\n",
    "e = multiply()\n",
    "print(e.forward_func)\n",
    "\n",
    "\n",
    "e = multiply(allow_arbitrary_many=True)\n",
    "print(e.forward_func)\n",
    "e = multiply(allow_arbitrary_many=False)\n",
    "print(e.forward_func)\n",
    "f = cos()\n",
    "print(f.forward_func(2), np.cos(2))  \n",
    "forw, backw = c.get_functions()\n",
    "print(forw, backw) \n",
    "ic.disable()\n",
    "ghj = ic(f.forward_func(2))\n",
    "ic(ghj)\n",
    "k = tanh()\n",
    "l = cos()\n",
    "ic.enable()\n",
    "ic(l.forward_func(2))\n",
    "ic()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79180ea9-fe5a-458a-9edf-9fe5b22261ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class expr_end_node():\n",
    "    def __init__(self, value: Number | np.ndarray , grad_value: Number | np.ndarray  = 0):\n",
    "        self.value = value\n",
    "        self.grad_value = grad_value\n",
    "        \n",
    "class expr_node():\n",
    "    def __init__(self, func: function, childs: Sequence[expr_node | expr_end_node] = []):\n",
    "        #self.parants = parants #expr_node\n",
    "        self.childs = childs  #expr_node\n",
    "        self.func = func #function \n",
    "        # lieber so:\n",
    "        # self.forward_func, self.backward_func = func.get_functions()\n",
    "        # und dann self.func entfernen\n",
    "        \n",
    "f = sin()\n",
    "g = sin()\n",
    "ex1 = expr_node(f)\n",
    "ex2 = expr_node(g)\n",
    "ex3 = expr_node(g)\n",
    "ex1.childs = [ex2]\n",
    "#ex2.parants = [ex1]\n",
    "ex2.childs = [ex3]\n",
    "#sin(sin(sin()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee800e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = expr_node(sin())\n",
    "ex2 = expr_node(sin())\n",
    "ex3 = expr_node(sin())\n",
    "x = expr_end_node(np.float64(0.345))\n",
    "ex1.childs = [ex2]\n",
    "ex2.childs = [ex3]\n",
    "ex3.childs = [x]\n",
    "\n",
    "#sin(sin(sin(x)))\n",
    "\"\"\"\n",
    "sin\n",
    " |\n",
    "sin\n",
    " |\n",
    "sin\n",
    " |\n",
    " x=0.345\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#cos(a)*sin(b + tanh(c))\n",
    "example_expr2 = expr_node(multiply(), \n",
    "                          [expr_node(cos(),\n",
    "                                     [expr_end_node(np.float64(0.345))]\n",
    "                                    ), \n",
    "                           expr_node(sin(),\n",
    "                                     [expr_node(add(), \n",
    "                                                [expr_end_node(np.float64(0.345)),\n",
    "                                                 expr_node(tanh(), \n",
    "                                                           [expr_end_node(np.float64(0.345))]\n",
    "                                                          )\n",
    "                                                ]\n",
    "                                               )\n",
    "                                     ]\n",
    "                                    )\n",
    "                          ]\n",
    "                         )\n",
    "\n",
    "# the same using the __call__ method of function (see above in function definition):\n",
    "example_expr3 = multiply()(\n",
    "                        cos()(\n",
    "                            expr_end_node(np.float64(0.345))),\n",
    "                            sin()(\n",
    "                                add()(\n",
    "                                    expr_end_node(np.float64(0.345)),\n",
    "                                    tanh()(\n",
    "                                        expr_end_node(np.float64(0.345))\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                )\n",
    "                \n",
    "\"\"\"\n",
    "    multiply\n",
    "    |      |\n",
    "   cos    sin\n",
    "   |       |\n",
    "  a=0.345 add\n",
    "          | |\n",
    "    b=0.345 tanh\n",
    "             |\n",
    "         c=0.345\n",
    "\"\"\"\n",
    "#short version of the sin(sin(sin(x))) exaple:\n",
    "x = expr_end_node(np.float64(0.345))\n",
    "example_expr1 = sin()( sin()( sin()( x ) ) ) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3932d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph_out/tt/the_graph.gv.pdf'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!source .jupyter_venv/.venv/bin/activate # source venv, if you have one for your kernal\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install graphviz\n",
    "\n",
    "import graphviz\n",
    "\n",
    "\n",
    "def print_graph(node, graph, parent_id=\"\"):\n",
    "    \n",
    "    node_id = str(id(node))\n",
    "    node_label = str(node.value) if isinstance(node, expr_end_node) else type(node.func).__name__\n",
    "    graph.node(node_id, node_label)  \n",
    "    \n",
    "    if parent_id:\n",
    "        graph.edge(parent_id, node_id)\n",
    "        \n",
    "    if hasattr(node, 'childs') and node.childs:\n",
    "            for child in node.childs:\n",
    "                print_graph(child, graph, node_id)#, constraint='false')\n",
    "\n",
    "\n",
    "dot1 = graphviz.Digraph('the_graph', comment='test') \n",
    "dot1.attr(rankdir=\"LR\")\n",
    "print_graph(example_expr3, dot1)\n",
    "#doctest_mark_exe() \n",
    "dot1.render(directory='graph_out/tt', view=True)\n",
    "#dot  \n",
    "#str(id(ex1))\n",
    "#type(ex1.func).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5ee0e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| forward(ex1): 0.32573257710464837\n",
      "ic| np.sin(np.sin(np.sin(np.float64(.345)))): 0.32573257710464837\n",
      "ic| x.grad_value: 0\n",
      "ic| np.cos(np.sin(np.sin(.345)))*np.cos(np.sin(.345))*np.cos(0.345): 0.8393506506551831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef print_all(a,b,c):\\n    print(a)\\n    print(b)\\n    print(c)\\n\\nprint_all(*[1,2,3])\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#f(g(h(x)))' = f'(g(h(x))) * (g(h(x)))' =  f'(g(h(x))) * g'(h(x)) * h'(x)\n",
    "# h'(x) * g'(h(x)) * f'(g(h(x)))\n",
    "\n",
    "\n",
    "def forward(node):\n",
    "    return node.func.forward_func(*(forward(child) for child in node.childs)) if type(node) is not expr_end_node else node.value \n",
    "forward(ex1)\n",
    "\"\"\"\n",
    "def forward(node):\n",
    "    if type(node) is not expr_end_node:\n",
    "    \n",
    "        node.func.forward_func(*(forward(child) for child in node.childs))\n",
    "    else:\n",
    "        node.value \n",
    "\"\"\"\n",
    "forward(ex1)\n",
    "\n",
    "def backward(node, value = 1):\n",
    "    if type(node) is not expr_end_node:\n",
    "        child_values = (forward(child) for child in node.childs)\n",
    "        if len(node.childs) == 1:\n",
    "            backward(node.childs[0], value * node.func.backward_func(*child_values))\n",
    "        else:\n",
    "            for child, new_value in zip(node.childs, node.func.backward_func(*child_values), strict=True):\n",
    "                backward(child, value * new_value)\n",
    "    else: \n",
    "        node.grad_value += value\n",
    "        \n",
    "        \n",
    "#result,target\n",
    "    \n",
    "        \n",
    "        \n",
    "x.grad_value=0\n",
    "\n",
    "ic(forward(ex1))\n",
    "ic(np.sin(np.sin(np.sin(np.float64(.345)))))\n",
    "\n",
    "backward(ex1)\n",
    "ic(x.grad_value)\n",
    "ic(np.cos(np.sin(np.sin(.345)))*np.cos(np.sin(.345))*np.cos(0.345))\n",
    "\n",
    "print(x.grad_value)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def print_all(a,b,c):\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)\n",
    "\n",
    "print_all(*[1,2,3])\n",
    "\"\"\"\n",
    "#zip((a1,a2.a3),(b1,b2,b3) -> ((a1,b1),(a2,b2),(a3,b3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dff8a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| iris.target: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 150\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0, 0, 0, 0, 0, 0, 0,\n",
      "                        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "                        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "                        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nout = vector len 3 , out_1: suit 1,  out_1: suit 2, \\nsuit_1 = (1,0,0)\\nsuit_2 = (0,1,0)\\nsuit_3 = (0,0,1)\\n\\nnetz = \\n    in: vecor len 4 \\n    out = vector len 3\\n    \\nwith  2-5 hidden layer; 4- 10 neurons each\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from numpy import linalg as LA\n",
    "\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "print(len(iris.data), len(iris.target))\n",
    "print(type(iris.data), type(iris.target))\n",
    "ic(iris.target)\n",
    "normalized_data = iris.data/LA.norm(iris.data, axis=0)\n",
    "\n",
    "\"\"\"\n",
    "out = vector len 3 , out_1: suit 1,  out_1: suit 2, \n",
    "suit_1 = (1,0,0)\n",
    "suit_2 = (0,1,0)\n",
    "suit_3 = (0,0,1)\n",
    "\n",
    "netz = \n",
    "    in: vecor len 4 \n",
    "    out = vector len 3\n",
    "    \n",
    "with  2-5 hidden layer; 4- 10 neurons each\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e956fc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| loss_mse(test_prediction, test_target): 0.29000000000000004\n",
      "ic| np.log(test_prediction): array([-1.60943791, -0.51082562, -1.2039728 ])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.60943791, -0.51082562, -1.2039728 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction = np.array([0.2, 0.6, 0.3])\n",
    "test_target = np.array([0, 1, 0])\n",
    "\n",
    "# Mean squared error (MSE):\n",
    "def loss_mse(prediction, target):\n",
    "    return np.square(prediction - target).sum()\n",
    "#(pred_x, pred_y), (target_x, target_y) -> (pred_x - target_x)²+(pred_y - target_y)²\n",
    "\n",
    "\n",
    "\n",
    "loss1 = ic(loss_mse(test_prediction, test_target))\n",
    "ic(np.log(test_prediction))\n",
    "# loss2 = ic(loss_ce(test_prediction,test_target))\n",
    "\n",
    "#ic.disable()\n",
    "#ic.enable()\n",
    "\n",
    "# 0.2²+0.4²+0.3² = \n",
    "#ic(loss1(test_prediction, test_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a782488b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninitial = w, b  are random values\\nnetz(in) = \\n    out_1 = in_1 * w_{1,1} + in_2 *w_{1,2} + b_1\\n    out_2 = in_1 * w_{2,1} + in_2 *w_{2,2} + b_2\\n\\n# net(in, {w}, {b}) \\nout = netz(in)\\nloss(out, true_val)\\nbackpropergation -> change  w, b <--------\\n\\n\\n\\ndict(np.sin: np.cos)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in_1     out_1\n",
    "# in_2     out_2\n",
    "\n",
    "# initial = w, b  are random values\n",
    "# netz(in) = \n",
    "#     out_1 = in_1 * w_{1,1} + in_2 *w_{1,2} + b_1\n",
    "#     out_2 = in_1 * w_{2,1} + in_2 *w_{2,2} + b_2\n",
    "\n",
    "# # net(in, {w}, {b}) \n",
    "# out = netz(in)\n",
    "# loss(out, true_val)\n",
    "# backpropergation -> change  w, b <--------\n",
    "\n",
    "\n",
    "\n",
    "# dict(np.sin: np.cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8711c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "# only as reference, should eventually be adjusted for our code \n",
    "# sigmoid changed to tanh\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = tanhyper(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            tanhyperprime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "\n",
    "def tanhyper(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanhyperprime(x):\n",
    "    return 1/cosh(x)**2\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
