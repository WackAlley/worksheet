{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cfa7556-fb78-4b4f-a68e-89138baa668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import alibi_positional_encoding\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformer import Transformer\n",
    "from beam_searcher import BeamSearcher\n",
    "from train_final_model import character_ecode_decode, read_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87c6f712-31d7-4ad8-babb-e1e98f0783a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"goe_full.txt\"\n",
    "full_text = read_in(file_path)\n",
    "encoder = character_ecode_decode(full_text)\n",
    "encoded_text = encoder.encode(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1d4c759-9a55-4e39-a8ec-46535c506937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Dies ist ein exakt einhundert Zeichen langer Text, der genau die gewünschte Länge einhält. Perfekt! \"\n",
    "input_text = \" Des Lebens Fluss strömt ewig fort, in Licht und Schatten wechselnd, gleich dem wandelnden Geschick.\" # 100\n",
    "input_text = \"Des Lebens Wogen steigen und sinken, \\\n",
    "ein ewiges Streben, ein flüchtiges Hoffen, \\\n",
    "gleich dem Wind, der durch kahle Zweige fährt, \\\n",
    "doch nimmer ruht; so irrt der Mensch, \\\n",
    "von Sehnsucht getrieben, vom Schicksal geführt, bis einst der Schleier fällt und Stille ihn\" # seq_length 256\n",
    "print(len(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9663811-d495-45f0-84a0-cca3399ddeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 256\n",
    "device = 'cpu' # torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_heads=8\n",
    "\n",
    "causal_padding_mask = torch.tril(torch.ones((seq_length, seq_length)))\n",
    "alibi_positional_encoding_tensor = alibi_positional_encoding(n_heads, seq_length)\n",
    "\n",
    "model = Transformer(\n",
    "        embedding_dim=80,\n",
    "        num_layers=12,\n",
    "        n_heads=n_heads, \n",
    "        device=device,\n",
    "        net_expansion_factor=4,\n",
    "        attention_type='dot_product',\n",
    "        alibi_bias=alibi_positional_encoding_tensor,\n",
    "        mask=causal_padding_mask\n",
    "    )\n",
    "\n",
    "searcher = BeamSearcher(\n",
    "        model = model,\n",
    "        device=device,\n",
    "        embedding_dimension = 80, #embedding_dim=70,\n",
    "        prediction_length = 40,\n",
    "        beam_width = 2,\n",
    "        max_candidates = 6,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ffc14a6d-1534-4d08-a9bf-5991a0474f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4643/4092577649.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        (\n",
    "            \"out/run1/Transformer,\"\n",
    "            \"lr_schedule=OneCycleLR, \"\n",
    "            \"loss_criterion=CrossEntropyLoss(), \"\n",
    "            \"net_expansion_factor=4, \"\n",
    "            \"n_layers=12, \"\n",
    "            \"n_heads=8, \"\n",
    "            \"attention_type=dot_product, \"\n",
    "            \"lr=0.001, \"\n",
    "            \"OneCycleLR, mask yes.pt\"\n",
    "        ),\n",
    "        map_location=device\n",
    "    )\n",
    ")\n",
    "\n",
    "encoded_text = torch.tensor([encoder.encode(input_text)], dtype=torch.int64)\n",
    "input_batch = model.one_hot_encode(encoded_text)\n",
    "searcher.do_search(input_batch)\n",
    "candidate_ranking = searcher.sequence_candidate_probabilities\n",
    "best_candidate = searcher.sequence_candidates[torch.argmax(candidate_ranking)]\n",
    "decoded_text = encoder.decode(model.one_hot_decode(best_candidate))\n",
    "model_1_prediction_beam_search = decoded_text[-(searcher.prediction_sequence_length):]\n",
    "# print(model_1_prediction_beam_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d087a024-d3f8-42f8-98cb-1d8f788d64c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4643/1772711987.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        (\n",
    "            \"out/run1/Transformer,\"\n",
    "            \"lr_schedule=OneCycleLR, \"\n",
    "            \"loss_criterion=CrossEntropyLoss(), \"\n",
    "            \"net_expansion_factor=4, \"\n",
    "            \"n_layers=12, \"\n",
    "            \"n_heads=8, \"\n",
    "            \"attention_type=dot_product, \"\n",
    "            \"lr=0.0005, \"\n",
    "            \"OneCycleLR, mask yes.pt\"\n",
    "        ),\n",
    "        map_location=device\n",
    "    )\n",
    ")\n",
    "\n",
    "encoded_text = torch.tensor([encoder.encode(input_text)], dtype=torch.int64)\n",
    "input_batch = model.one_hot_encode(encoded_text)\n",
    "searcher.do_search(input_batch)\n",
    "candidate_ranking = searcher.sequence_candidate_probabilities\n",
    "best_candidate = searcher.sequence_candidates[torch.argmax(candidate_ranking)]\n",
    "decoded_text = encoder.decode(model.one_hot_decode(best_candidate))\n",
    "model_2_prediction_beam_search = decoded_text[-(searcher.prediction_sequence_length):]\n",
    "# print(model_2_prediction_beam_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5dad7452-c59b-4d9d-a8f1-a320f2be5667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4643/753965067.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        (\n",
    "            \"out/run2/Transformer,\"\n",
    "            \"lr_schedule=OneCycleLR, \"\n",
    "            \"loss_criterion=CrossEntropyLoss(), \"\n",
    "            \"net_expansion_factor=4, \"\n",
    "            \"n_layers=12, \"\n",
    "            \"n_heads=8, \"\n",
    "            \"attention_type=dot_product, \"\n",
    "            \"lr=0.0001, \"\n",
    "            \"OneCycleLR, mask yes.pt\"\n",
    "        ),\n",
    "        map_location=device\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "encoded_text = torch.tensor([encoder.encode(input_text)], dtype=torch.int64)\n",
    "input_batch = model.one_hot_encode(encoded_text)\n",
    "searcher.do_search(input_batch)\n",
    "candidate_ranking = searcher.sequence_candidate_probabilities\n",
    "best_candidate = searcher.sequence_candidates[torch.argmax(candidate_ranking)]\n",
    "decoded_text = encoder.decode(model.one_hot_decode(best_candidate))\n",
    "model_3_prediction_beam_search = decoded_text[-(searcher.prediction_sequence_length):]\n",
    "# print(model_3_prediction_beam_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87776372-92b4-4ab6-8fde-3a89efc78816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4643/2418309612.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        (\n",
    "            \"out/run2/Transformer,\"\n",
    "            \"lr_schedule=OneCycleLR, \"\n",
    "            \"loss_criterion=CrossEntropyLoss(), \"\n",
    "            \"net_expansion_factor=4, \"\n",
    "            \"n_layers=12, \"\n",
    "            \"n_heads=8, \"\n",
    "            \"attention_type=dot_product, \"\n",
    "            \"lr=0.0005, \"\n",
    "            \"OneCycleLR, mask yes.pt\"\n",
    "        ),\n",
    "        map_location=device\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "encoded_text = torch.tensor([encoder.encode(input_text)], dtype=torch.int64)\n",
    "input_batch = model.one_hot_encode(encoded_text)\n",
    "searcher.do_search(input_batch)\n",
    "candidate_ranking = searcher.sequence_candidate_probabilities\n",
    "best_candidate = searcher.sequence_candidates[torch.argmax(candidate_ranking)]\n",
    "decoded_text = encoder.decode(model.one_hot_decode(best_candidate))\n",
    "model_4_prediction_beam_search = decoded_text[-(searcher.prediction_sequence_length):]\n",
    "# print(model_4_prediction_beam_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d028b147-d127-42ec-beb9-e0506c24f90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4643/4228952444.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        (\n",
    "            \"out/run3/Transformer,\"\n",
    "            \"lr_schedule=OneCycleLR, \"\n",
    "            \"loss_criterion=CrossEntropyLoss(), \"\n",
    "            \"net_expansion_factor=4, \"\n",
    "            \"n_layers=12, \"\n",
    "            \"n_heads=8, \"\n",
    "            \"attention_type=dot_product, \"\n",
    "            \"lr=0.0005, \"\n",
    "            \"OneCycleLR, mask yes.pt\"\n",
    "        ),\n",
    "        map_location=device\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "encoded_text = torch.tensor([encoder.encode(input_text)], dtype=torch.int64)\n",
    "input_batch = model.one_hot_encode(encoded_text)\n",
    "searcher.do_search(input_batch)\n",
    "candidate_ranking = searcher.sequence_candidate_probabilities\n",
    "best_candidate = searcher.sequence_candidates[torch.argmax(candidate_ranking)]\n",
    "decoded_text = encoder.decode(model.one_hot_decode(best_candidate))\n",
    "model_5_prediction_beam_search = decoded_text[-(searcher.prediction_sequence_length):]\n",
    "# print(model_5_prediction_beam_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d417b1e-f408-41b0-9176-81c04baca926",
   "metadata": {},
   "source": [
    "| Model  | Learning Rate | Epochs | Layers | Sequence Length |\n",
    "|--------|--------------|--------|--------|--|\n",
    "| Model 1 | 0.001        | 2      | 12      | 100 |\n",
    "| Model 2 | 0.0005       | 4      | 12      | 100 |\n",
    "| Model 3 | 0.0001       | 2     | 12      | 256 |\n",
    "| Model 4 | 0.0005       | 2      | 12      | 256 |\n",
    "| Model 5 | 0.0005       | 7      | 12      | 256 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12894ac6-d27a-4686-aa10-c80aa9a63bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Des Lebens Wogen steigen und sinken, ein ewiges Streben, ein flüchtiges Hoffen, gleich dem Wind, der durch kahle Zweige fährt, doch nimmer ruht; so irrt der Mensch, von Sehnsucht getrieben, vom Schicksal geführt, bis einst der Schleier fällt und Stille ihn\n",
      "...\n",
      "Model 1:  , daß ich dieser Schiffen unter der Schi\n",
      "Model 2:   einer groß und die Schreitten und die S\n",
      "Model 3:   der der derer sich der sich der sichte \n",
      "Model 4:   der Gesten der Geschaften sich der Gesc\n",
      "Model 5:   seinen Geschichten und die Geschichten \n"
     ]
    }
   ],
   "source": [
    "print(input_text)\n",
    "print(\"...\")\n",
    "print(\"Model 1: \",model_1_prediction_beam_search)\n",
    "print(\"Model 2: \",model_2_prediction_beam_search)\n",
    "print(\"Model 3: \",model_3_prediction_beam_search)\n",
    "print(\"Model 4: \",model_4_prediction_beam_search)\n",
    "print(\"Model 5: \",model_5_prediction_beam_search)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
